{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e12942",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde6a4b",
   "metadata": {},
   "source": [
    "### Model overview\n",
    "\n",
    "#### Assumptions\n",
    "* Neuron values are either 0 or 1 (binary). \n",
    "* Weights are real values, which are maybe bounded. \n",
    "* Fixed bias to fix ideas (subject to change).\n",
    "\n",
    "#### Architecture\n",
    "Let $H(x)$ be the binary step function \n",
    "$$\n",
    "H(x):=\\begin{cases}1 \\quad x\\geq 0\\\\0 \\quad x< 0.\\end{cases}\n",
    "$$\n",
    "\n",
    "The *forward pass* of one layer reads \n",
    "$$\n",
    "L_i(x,w) := H\\left( \\sum_{j}  w[i,j]\\cdot x[j] - B \\right),\n",
    "$$\n",
    "where $x$ are the inputs, or neuron values, and $w$ are the weights, that is the synaptic values. We denote by $B$ the bias, it is a positive constant.\n",
    "\n",
    "From inputs to final outputs with $n$ layers, the forward pass looks like \n",
    "$$\n",
    "x_0 \\xrightarrow{w_0} x_1 \\xrightarrow{w_1} x_2 \\rightarrow \\dots \\rightarrow x_{n-1} \\xrightarrow{w_{n-1}} x_n,\n",
    "$$\n",
    "with \n",
    "$$\n",
    "x_i = L(x_{i-1}, w_{i-1}), \\quad x_0 = \\text{given inputs}.\n",
    "$$\n",
    "Again, $x$ has only entries in $\\{0, 1\\}$, and $w$ has floats as entries (bounded?).  \n",
    "\n",
    "The *backward pass* is given by\n",
    "$$\n",
    "gx_n \\xrightarrow{gw_{n-1}} gx_{n-1} \\xrightarrow{gw_{n-2}} gx_{n-2} \\rightarrow \\dots \\rightarrow gx_{1} \\xrightarrow{gw_{0}} gx_0,\n",
    "$$\n",
    "where \n",
    "\\begin{split}\n",
    "gx_i &= G(gx_{i+1}, x_{i+1}, x_{i}, w_{i})\\\\\n",
    "gw_i &= W(gx_{i+1}, x_{i+1}, x_i) \\\\\n",
    "gx_n &= Neq(\\text{desired outputs}, x_n).\n",
    "\\end{split}\n",
    "We will define $G$, $W$, and $Neq$ in the following. \n",
    "\n",
    "As for the forward pass, $gx$ has entries in $\\{0, 1\\}$, and $gw$ has floats as entries. Moreover, the shape of $x_i$ equals the shape of $gx_i$, and the shape of $w_i$ equals the shape of $gw_i$.\n",
    "\n",
    "### Backward pass formulas\n",
    "\n",
    "#### Neq\n",
    "For $x, y \\in \\{0, 1\\}$, we set\n",
    "$$\n",
    "Neq(x, y) = \\begin{cases} 0 \\quad  &x=y, \\\\ 1 \\quad &x\\neq y. \\end{cases} \n",
    "$$\n",
    "This is generalized to $x$ and $y$ of the same shape by applying entry by entry. The signature is\n",
    "```\n",
    "Neq: (bool, shape) x (bool, shape) -> (bool, shape)  \n",
    "```\n",
    "\n",
    "#### W\n",
    "Let $n$ be the shape of $x$ and $gx$, and let $m$ be the shape of $x'$. We define $W(gx, x, x')$, which is of shape $n\\times m$, by \n",
    "$$\n",
    "W(gx, x, x') := \\left( gx[i] \\cdot P(x[i], x'[j]) \\right)_{i,j},\n",
    "$$\n",
    "where \n",
    "$$\n",
    "P(0, 1) = 1, \\quad P(1, 1) = -1, \\quad P(0, 0) = Q, \\quad P(1, 0) = -Q,  \n",
    "$$\n",
    "with a non-negative $Q<1$. The value of $Q$ is connected to the learning rate (reference???). In order to compute $P$, we can use \n",
    "$$\n",
    "P(x,y) =  (1 - 2 \\cdot x) \\cdot (Q  + (1 - Q) \\cdot y) \n",
    "$$ \n",
    "\n",
    "The signature is\n",
    "```\n",
    "W: (bool, n) x (bool, n) x (bool, m) -> ({-1, -Q, 0, Q, 1}, n x m).  \n",
    "```\n",
    "\n",
    "#### G\n",
    "Let $n$ be the shape of $x$ and $gx$, and $m$ the shape of $x'$, then $w'$ must have shape $n\\times m$. We define \n",
    "$$\n",
    "G(gx, x, x', w') = \\left(H\\left( \\sum_{i} gx[i]\\cdot Seq(x[i], x'[j])  \\cdot w[i,j] - B\\right) \\right)_{j},\n",
    "$$\n",
    "where \n",
    "$$\n",
    "Seq(x,y) = (2x-1)\\cdot (2y-1)  = \\begin{cases} 1 \\quad  &x=y, \\\\ -1 \\quad &x\\neq y. \\end{cases}\n",
    "$$\n",
    "The signature is \n",
    "```\n",
    "G: (bool, n) x (bool, n) x (bool, m) x (float, n x m) -> (bool, m).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2e1ad",
   "metadata": {},
   "source": [
    "### Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11805e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0632c8f",
   "metadata": {},
   "source": [
    "#### Heaviside "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46b93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([-1, 0, 1], dtype=torch.float32)\n",
    "value = torch.tensor([1], dtype=torch.float32)\n",
    "\n",
    "def H(input):\n",
    "    return torch.heaviside(input, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b9c9d",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a944572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2000,  0.1000,  1.1000],\n",
       "         [ 0.0000, -0.4000,  0.3000]], requires_grad=True),\n",
       " tensor([0., 1., 1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([[0.2, 0.1, 1.1], [0, -0.4, 0.3]], dtype=torch.float32, requires_grad=True)\n",
    "x = torch.tensor([0, 1, 1], dtype=torch.float32)\n",
    "weights, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e972dc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2000, -0.1000], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(weights, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3aa5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_bool(weights, input, B):\n",
    "    return H(torch.matmul(weights, input) - B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc7fa2",
   "metadata": {},
   "source": [
    "#### Neq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89f96e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 1.]]), tensor([[0., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[0, 1, 1, 1]], dtype=torch.float32)\n",
    "s = torch.tensor([[0, 0, 0, 1]], dtype=torch.float32)\n",
    "s, t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f51f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neq(s,t):\n",
    "    return 1-(s==t).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21500d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neq(s,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48ed2a1",
   "metadata": {},
   "source": [
    "#### W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5a8637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1.]), tensor([1., 1., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0,1], dtype=torch.float32)\n",
    "y = torch.tensor([1,1,0], dtype=torch.float32, requires_grad=True)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44eaf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1., -1.]), tensor([1.0000, 1.0000, 0.8000], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.tensor([0.8], dtype=torch.float32)\n",
    "\n",
    "first_factor = 1 - 2*x\n",
    "second_factor = Q + (1-Q) * y\n",
    "first_factor, second_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "992b6c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  0.8000],\n",
       "        [-1.0000, -1.0000, -0.8000]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(torch.reshape(first_factor, first_factor.shape+(1,)),  \n",
    "         torch.reshape(second_factor, (1,)+second_factor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db29d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in gx\n",
    "gx = torch.tensor([1,0], dtype=torch.float32)\n",
    "first_factor = gx * (1 - 2*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33bc67e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.8000],\n",
       "        [-0.0000, -0.0000, -0.0000]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(torch.reshape(first_factor, first_factor.shape+(1,)),  \n",
    "         torch.reshape(second_factor, (1,)+second_factor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f187991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W(gx, x, y, q):\n",
    "    first_factor = gx * (1 - 2*x)\n",
    "    second_factor = q + (1-q) * y\n",
    "    return torch.mm(torch.reshape(first_factor, first_factor.shape+(1,)),  \n",
    "             torch.reshape(second_factor, (1,)+second_factor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44b86f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.8000],\n",
       "        [-0.0000, -0.0000, -0.0000]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W(gx,x,y,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a60c837",
   "metadata": {},
   "source": [
    "#### G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ec62911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.,  1.]), tensor([ 1.,  1., -1.], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0,1], dtype=torch.float32)\n",
    "y = torch.tensor([1,1,0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "seq_first_factor = 2*x - 1\n",
    "seq_second_factor = 2*y - 1\n",
    "seq_first_factor, seq_second_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14f16ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq(x, y):\n",
    "    seq_first_factor = 2*x - 1\n",
    "    seq_second_factor = 2*y - 1\n",
    "    return torch.mm(torch.reshape(seq_first_factor, seq_first_factor.shape+(1,)),  \n",
    "             torch.reshape(seq_second_factor, (1,)+seq_second_factor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb883a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1.,  1.],\n",
       "        [ 1.,  1., -1.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dfa3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(gx, x, y, w, B):\n",
    "    new_w = seq(x, y) * w \n",
    "    return H(torch.matmul(gx, new_w) - B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a383cc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gx : tensor([1., 0.]) \n",
      " x: tensor([0., 1.]) \n",
      " y: tensor([1., 1., 0.], requires_grad=True)\n",
      " w: tensor([[ 0.2000,  0.1000,  1.1000],\n",
      "        [ 0.0000, -0.4000,  0.3000]], requires_grad=True) \n",
      " B: tensor([0.5000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.], grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([0.5])\n",
    "print(\"gx : {} \\n x: {} \\n y: {}\\n w: {} \\n B: {}\".format(gx, x, y, weights, B))\n",
    "G(gx, x, y, weights, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a5be5",
   "metadata": {},
   "source": [
    "#### Autograd function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e877099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBool(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    # B and Q are added as tensors\n",
    "    def forward(ctx, input, weight, b, q):\n",
    "        output = forward_bool(weight, input, b)\n",
    "        ctx.save_for_backward(input, weight, b, q, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, b, q, output = ctx.saved_tensors\n",
    "\n",
    "        grad_input = grad_weight = grad_b = grad_q = None\n",
    "\n",
    "        grad_input = G(grad_output, output, input, weight, b)\n",
    "        grad_weight = W(grad_output, output, input, q)\n",
    "\n",
    "        return grad_input, grad_weight, grad_b, grad_q\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1934a20",
   "metadata": {},
   "source": [
    "##### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c268ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.], grad_fn=<LinearBoolBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward\n",
    "\n",
    "linear = LinearBool.apply\n",
    "#print(y, weights, B, Q)\n",
    "output = linear(y, weights, B, Q)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb89903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward\n",
    "desired_output = torch.tensor([1, 1], dtype=torch.float32)\n",
    "gradient = neq(output, desired_output)\n",
    "\n",
    "output.backward(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bd494e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000, 0.8000],\n",
       "         [1.0000, 1.0000, 0.8000]]),\n",
       " tensor([0., 0., 1.]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0c007",
   "metadata": {},
   "source": [
    "#### Learning tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4543e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def learn(inputs, desired_outputs, model, weights, bias, learning_rate, \n",
    "            binarizatio_fn=lambda input: torch.heaviside(input, torch.tensor([0.])),\n",
    "            loss_gradient = neq):\n",
    "\n",
    "    for w in weights:\n",
    "        w.grad = None\n",
    "\n",
    "    for b in bias:\n",
    "        b.grad = None\n",
    "\n",
    "    eq_counter = 0\n",
    "    for i, o in zip(inputs, desired_outputs):\n",
    "        output = model(i)\n",
    "        output_bn = binarizatio_fn(output)\n",
    "\n",
    "        if torch.all(o==output_bn):\n",
    "            eq_counter += 1\n",
    "        else:\n",
    "            gradient = loss_gradient(o, output_bn)\n",
    "            \n",
    "            output.backward(gradient)\n",
    "            with torch.no_grad():\n",
    "                for w in weights:\n",
    "                    w += learning_rate * w.grad\n",
    "                    w.grad = None\n",
    "                for b in bias:\n",
    "                    b += learning_rate * b.grad\n",
    "                    b.grad = None\n",
    "                    \n",
    "    if eq_counter == len(inputs):\n",
    "        return \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c1282",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "In order to compare performance with a non binary approach, we introduce a simple linear layer with constant bias and ReLU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ceae815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_compare(y, weights, B):\n",
    "    l = torch.matmul(weights, y) - B\n",
    "    return torch.nn.ReLU()(l)\n",
    "\n",
    "def loss_gradient(o, output_bn):\n",
    "    return o-output_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe65a9c",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "All examples start with random initialisation of weights in the range $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15325304",
   "metadata": {},
   "source": [
    "#### Copy one neuron\n",
    "\n",
    "Architecture: `input_neuron -> output_neuron` \n",
    "\n",
    "Desired behavior: `0->0, 1->1`\n",
    "\n",
    "Explicitly, for forward $y=H(x\\cdot w-B)$. If $x=0$ then $y=0$, which is desired behavior. Suppose $x=1$ and $y=0$. Desired value is $y'=1$. For backward: the first gradient is $gy= Eq(y', y)=1$. The weight gradient reads $gw = W(gy, y, x)= 1$, pointing to increasing $w$, which is correct behavior. \n",
    "\n",
    "Once correct behavior is established, the weight gradients are zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "202e1e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with weight tensor([-0.6000], requires_grad=True)\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradient: tensor([1.])\n",
      "Changing weight to tensor([0.4000], requires_grad=True)\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradient: tensor([1.])\n",
      "Changing weight to tensor([1.4000], requires_grad=True)\n",
      "Output: tensor([1.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradient: tensor([0.])\n",
      "Changing weight to tensor([1.4000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1.], requires_grad=True)\n",
    "desired_output = torch.tensor([1.], requires_grad=True)\n",
    "# start with negative weight, it will become positive after learning \n",
    "w = torch.tensor([-0.6], requires_grad=True)\n",
    "print(\"Starting with weight {}\".format(w))\n",
    "gradient = None\n",
    "limit = 10\n",
    "counter = 0\n",
    "\n",
    "while gradient != torch.tensor([0.]) and counter<limit:\n",
    "    output = linear(y, w, B, Q)\n",
    "    print(\"Output: {}\".format(output))\n",
    "\n",
    "    gradient = neq(output, desired_output)\n",
    "    output.backward(gradient)\n",
    "    print(\"Weight gradient: {}\".format(w.grad))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w += w.grad\n",
    "    print(\"Changing weight to {}\".format(w))\n",
    "    w.grad = None\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae87f7c",
   "metadata": {},
   "source": [
    "#### Copy one neuron with a neuron in between\n",
    "\n",
    "Architecture: `input_neuron -> other_neuron -> output_neuron` \n",
    "\n",
    "Desired behavior: `0->0, 1->1`\n",
    "\n",
    "The backpropagation of \n",
    "$$\n",
    "x_\\text{in} \\xrightarrow{w_\\text{in}} x_\\text{other} \\xrightarrow{w_\\text{other}} x_{\\text{out}}\n",
    "$$\n",
    "is computed as follows\n",
    "$$\n",
    "gx_\\text{out} \\xrightarrow{gw_\\text{other}} gx_\\text{other} \\xrightarrow{gw_\\text{in}} gx_\\text{in}$$\n",
    "Independent of the $x_{\\text{other}}$ values, $gw_\\text{other}$ will be positive if the desired value for $x_\\text{out}$ is not reached. Once $w_\\text{other}$ is large enough, $gx_\\text{other}$ will become positive if $x_\\text{out}=0$ is not desired and $x_{\\text{other}}=0$. This will push $gw_\\text{in}$ to positive, fixing $w_\\text{in}$. The model will solve the task.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30c7f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with weights [tensor([-0.6000], requires_grad=True), tensor([-0.9000], requires_grad=True)]\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradients: [tensor([0.]), tensor([0.8000])]\n",
      "Changing weight to [tensor([-0.6000], requires_grad=True), tensor([-0.1000], requires_grad=True)]\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradients: [tensor([0.]), tensor([0.8000])]\n",
      "Changing weight to [tensor([-0.6000], requires_grad=True), tensor([0.7000], requires_grad=True)]\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradients: [tensor([1.]), tensor([0.8000])]\n",
      "Changing weight to [tensor([0.4000], requires_grad=True), tensor([1.5000], requires_grad=True)]\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradients: [tensor([1.]), tensor([0.8000])]\n",
      "Changing weight to [tensor([1.4000], requires_grad=True), tensor([2.3000], requires_grad=True)]\n",
      "Output: tensor([1.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradients: [tensor([0.]), tensor([0.])]\n",
      "Changing weight to [tensor([1.4000], requires_grad=True), tensor([2.3000], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1.], requires_grad=True)\n",
    "desired_output = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "# start with negative weight, it will become positive after learning \n",
    "w_in = torch.tensor([-0.6], requires_grad=True)\n",
    "w_other = torch.tensor([-0.9], requires_grad=True)\n",
    "weights = [w_in, w_other]\n",
    "print(\"Starting with weights {}\".format(weights))\n",
    "\n",
    "def model(input): \n",
    "    return linear(linear(input, w_in, B, Q), w_other, B, Q)\n",
    "\n",
    "gradient = None\n",
    "limit = 10\n",
    "counter = 0\n",
    "\n",
    "while gradient != torch.tensor([0.]) and counter < limit:\n",
    "    output = model(y)\n",
    "    print(\"Output: {}\".format(output))\n",
    "\n",
    "    gradient = neq(output, desired_output)\n",
    "    output.backward(gradient)\n",
    "    print(\"Weight gradients: {}\".format([w.grad for w in weights]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for w in weights:\n",
    "            w += w.grad\n",
    "            w.grad = None            \n",
    "    print(\"Changing weight to {}\".format(weights))\n",
    " \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d241950",
   "metadata": {},
   "source": [
    "#### Always disagree with one neuron\n",
    "\n",
    "Architecture: `input_neuron -> output_neuron` \n",
    "\n",
    "Desired behavior: `1->0`. Note that `0->0` is automatic. \n",
    "\n",
    "Explicitly, for forward $y=H(x\\cdot w-B)$. If $x=1$ and $y=1$, then the weight gradient is\n",
    "$\n",
    "gw = -1\n",
    "$\n",
    "pushing the weights in the negative direction, which is correct behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a598200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New weights: [tensor([0.5000], requires_grad=True)]\n",
      "[1.] -> [0.]\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([1.], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([0.])]\n",
    "weights = [torch.tensor([1.], requires_grad=True)]\n",
    "\n",
    "def model(input):\n",
    "    return linear(input, weights[0], B, Q)\n",
    "\n",
    "for i in range(10):\n",
    "    learn(inputs, desired_outputs, model, weights, [], learning_rate=0.1)\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f1f37",
   "metadata": {},
   "source": [
    "#### Switch positions\n",
    "\n",
    "Architecture: `input_neuron_0, input_neuron_1  -> output_neuron_0, output_neuron_1` \n",
    "\n",
    "Desired behavior: `I:(1,0)->(0,1), II:(0,1)->(1,0), III:(1,1)->(1,1)`\n",
    "\n",
    "Explicitly, for forward $y_i=H(x_0\\cdot w_{i,0} + x_1\\cdot w_{i,1} - B)$. Let's consider the weights one by one:\n",
    "* $w_{0,0}$: since `I` and `III` send contradictory impulses at first, no changes.\n",
    "* $w_{0,1}$: increasing, so it will fix `II` and have a strong angle on `III`. This will allow $w_{0,0}$ to focus on `I` and fix it. \n",
    "* $w_{1,0}$: Same as $w_{0,1}$.\n",
    "* $w_{1,1}$: Same as $w_{0,0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3014401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done after epoch 11\n",
      "New weights: [tensor([[0.4800, 0.5400],\n",
      "        [0.5800, 0.4900]], requires_grad=True)]\n",
      "[1. 0.] -> [0. 1.]\n",
      "[0. 1.] -> [1. 0.]\n",
      "[1. 1.] -> [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([1., 0.], requires_grad=True), \n",
    "          torch.tensor([0., 1.], requires_grad=True),\n",
    "          torch.tensor([1., 1.], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([0., 1.]), torch.tensor([1., 0.]), torch.tensor([1., 1.])]\n",
    "\n",
    "# initialization must be random\n",
    "weights = [torch.tensor([[0.1, -0.2], \n",
    "                         [-0.22, -0.01]], requires_grad=True)]\n",
    "\n",
    "def model(input):\n",
    "    return linear(input, weights[0], B, Q)\n",
    "\n",
    "for i in range(12):\n",
    "    done = learn(inputs, desired_outputs, model, weights, [],  learning_rate=0.1)\n",
    "    if done==\"Done\":\n",
    "        print(\"Done after epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b126fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done after epoch 12\n",
      "New weights: [tensor([[0.1000, 0.2000],\n",
      "        [0.2000, 0.1000]], requires_grad=True)]\n",
      "New bias: []\n",
      "[1. 0.] -> [0. 1.]\n",
      "[0. 1.] -> [1. 0.]\n",
      "[1. 1.] -> [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Comparison with ReLUs \n",
    "\n",
    "# initialization must be random\n",
    "# Following weights won't work, because ReLU does not propagate grads if value = 0\n",
    "# weights = [torch.tensor([[0.1, -0.2], \n",
    "#                         [-0.22, -0.01]], requires_grad=True)]\n",
    "weights = [torch.tensor([[0.1, 0.13], \n",
    "                            [0.05, 0.07]], requires_grad=True)]\n",
    "\n",
    "#bias = [torch.tensor([0.5], requires_grad=True)]\n",
    "bias = []\n",
    "binarization_fn = lambda input: torch.heaviside(input-0.2, torch.tensor([0.]))\n",
    "\n",
    "def model(input):\n",
    "    return linear_compare(input, weights[0], torch.tensor([0.])) \n",
    "\n",
    "for i in range(20):\n",
    "    done = learn(inputs, desired_outputs, model, weights, bias, learning_rate=0.01, \n",
    "                    binarizatio_fn=binarization_fn, loss_gradient=loss_gradient)\n",
    "    #done = learn(inputs, desired_outputs, model, weights, bias, learning_rate=0.1)\n",
    "    if done==\"Done\":\n",
    "        print(\"Done after epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "print(\"New bias: {}\".format(bias))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), binarization_fn(model(input)).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94f3f7",
   "metadata": {},
   "source": [
    "#### Hidden representation\n",
    "\n",
    "Architecture: `input_0, input_1  -> hidden_0, hidden_1, hidden_2 -> output` \n",
    "\n",
    "Desired behavior: `I:(1,0)->(1), II:(0,1)->(1), III:(1,1)->(0)`\n",
    "\n",
    "This cannot be learned with one layer only, because `I` and `II` imply two weights that are larger than the bias. This contradicts `III`.\n",
    "\n",
    "TODO\n",
    "\n",
    "* If `learnig rate` is a multiple of `Q` or vice versa, then desired behaviour is not learned. Explanation?\n",
    "* If `Q = 0` then desired behaviour is not learned. Explanation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed424cc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "learn() missing 1 required positional argument: 'bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/andre/papers/binary_nn/theory.ipynb Cell 50'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/andre/papers/binary_nn/theory.ipynb#ch0000052vscode-remote?line=18'>19</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/andre/papers/binary_nn/theory.ipynb#ch0000052vscode-remote?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/andre/papers/binary_nn/theory.ipynb#ch0000052vscode-remote?line=21'>22</a>\u001b[0m     done \u001b[39m=\u001b[39m learn(inputs, desired_outputs, model, weights, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.11\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/andre/papers/binary_nn/theory.ipynb#ch0000052vscode-remote?line=22'>23</a>\u001b[0m     \u001b[39mif\u001b[39;00m done \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/andre/papers/binary_nn/theory.ipynb#ch0000052vscode-remote?line=23'>24</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished in epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i))\n",
      "\u001b[0;31mTypeError\u001b[0m: learn() missing 1 required positional argument: 'bias'"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([1., 0.], requires_grad=True), \n",
    "          torch.tensor([0., 1.], requires_grad=True),\n",
    "          torch.tensor([1., 1.], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([1.]), torch.tensor([1.]), torch.tensor([0.])]\n",
    "\n",
    "use_Q = torch.tensor([0.1])\n",
    "\n",
    "# initialization must be random\n",
    "weights = [torch.tensor([[0.1, -0.2], \n",
    "                         [-0.22, -0.01],\n",
    "                         [0.22, 0.01]], requires_grad=True), \n",
    "           torch.tensor([0.1, 0.16, -0.2], requires_grad=True)]\n",
    "\n",
    "def model(input):\n",
    "    hidden = linear(input, weights[0], B, use_Q)\n",
    "    output = linear(hidden, weights[1], B, use_Q)\n",
    "    return output\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for i in range(epochs):\n",
    "    done = learn(inputs, desired_outputs, model, weights, [], learning_rate=0.11)\n",
    "    if done == \"Done\":\n",
    "        print(\"Finished in epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c7025",
   "metadata": {},
   "source": [
    "#### Hidden representation 2\n",
    "\n",
    "Architecture: `input_0, input_1  -> hidden_0, hidden_1 -> output` \n",
    "\n",
    "Desired behavior: `I:(1,0)->(1), II:(0,1)->(1), III:(1,1)->(0)`\n",
    "\n",
    "This cannot be learned with one layer only, because `I` and `II` imply two weights that are larger than the bias. This contradicts `III`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91707287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in epoch 143\n",
      "New weights: [tensor([[0.2072, 0.2010],\n",
      "        [0.1923, 0.0900]], requires_grad=True), tensor([ 0.2015, -0.0040], requires_grad=True)]\n",
      "[1. 0.] -> [1.]\n",
      "[0. 1.] -> [1.]\n",
      "[1. 1.] -> [0.]\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([1., 0.], requires_grad=True), \n",
    "          torch.tensor([0., 1.], requires_grad=True),\n",
    "          torch.tensor([1., 1.], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([1.]), torch.tensor([1.]), torch.tensor([0.])]\n",
    "\n",
    "use_Q = torch.tensor([0.115])\n",
    "\n",
    "# initialization must be random\n",
    "weights = [torch.tensor([[0.1, 0.2], \n",
    "                         [0.22, 0.1]], requires_grad=True), \n",
    "           torch.tensor([0.1, 0.16], requires_grad=True)]\n",
    "\n",
    "def model(input, B=torch.tensor([0.2])):\n",
    "    hidden = linear(input, weights[0], B, use_Q)\n",
    "    output = linear(hidden, weights[1], B, use_Q)\n",
    "    return output\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for i in range(epochs):\n",
    "    done = learn(inputs, desired_outputs, model, weights, [], learning_rate=0.01)\n",
    "    if done == \"Done\":\n",
    "        print(\"Finished in epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in epoch 1572\n",
      "New weights: [tensor([[-0.4186,  0.4077],\n",
      "        [ 0.4946, -0.0263]], requires_grad=True), tensor([0.5427, 0.4234], requires_grad=True)]\n",
      "[1. 0.] -> [1.]\n",
      "[0. 1.] -> [1.]\n",
      "[1. 1.] -> [0.]\n"
     ]
    }
   ],
   "source": [
    "### Comparison with ReLUs\n",
    "\n",
    "inputs = [torch.tensor([1., 0.], requires_grad=True), \n",
    "          torch.tensor([0., 1.], requires_grad=True),\n",
    "          torch.tensor([1., 1.], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([1.]), torch.tensor([1.]), torch.tensor([0.])]\n",
    "\n",
    "bias = []\n",
    "binarization_fn = lambda input: torch.heaviside(input-0.2, torch.tensor([0.]))\n",
    "\n",
    "# initialization must be random\n",
    "weights = [torch.tensor([[0.1, 0.2], \n",
    "                         [0.22, 0.1]], requires_grad=True), \n",
    "           torch.tensor([0.1, 0.16], requires_grad=True)]\n",
    "\n",
    "zero = torch.tensor([0.])\n",
    "\n",
    "def model(input):\n",
    "    hidden = linear_compare(input, weights[0], zero)\n",
    "    output = linear_compare(hidden, weights[1], zero)\n",
    "    return output\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for i in range(epochs):\n",
    "    done = learn(inputs, desired_outputs, model, weights, bias, learning_rate=0.05,\n",
    "                    binarizatio_fn=binarization_fn, loss_gradient=loss_gradient)\n",
    "    if done == \"Done\":\n",
    "        print(\"Finished in epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), binarization_fn(model(input)).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c141c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 1.12 Kernel",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
