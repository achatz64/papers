{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e12942",
   "metadata": {},
   "source": [
    "## Leaky binary models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bf12a",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "* Neuron values are either 0 or 1 (binary). The inputs and outputs for sure, and internally as much as possible.\n",
    "* Weights are real values, which are maybe bounded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c404ca3",
   "metadata": {},
   "source": [
    "#### Modified Heaviside\n",
    "\n",
    "The Heaviside function is defined as \n",
    "$$\n",
    "H(x):=\\begin{cases}1 \\quad x\\geq 0\\\\0 \\quad x< 0.\\end{cases}\n",
    "$$\n",
    "Mathematically, it has vanishing differential, because $H$ is essentially constant. \n",
    "\n",
    "<!-- We will use the following leaky variant of $H$, which depends on a non-negative constant $Q$:\n",
    "$$\n",
    "H(x, Q):= \\begin{cases} 1 \\quad x\\geq 0\\\\ Q \\quad x< 0.\\end{cases}\n",
    "$$ -->\n",
    "For our purpose, it will be convenient to use the following differential for $H$, \n",
    "$$\n",
    "H_{\\text{backward}} := \\begin{cases} 1 \\quad x\\geq 0\\\\ -1 \\quad x< 0.\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77a818",
   "metadata": {},
   "source": [
    "#### Internal binary representation\n",
    "\n",
    "In order to use classical float based layers for binary computations, we will use a leaky variant of a binary tensor. Instead of $[1, 0, 0, 1]$ we will use $[1, Q, Q, 1]$, where $Q<1$ is a non-negative constant, which is typically small. This will help to overcome a stop of the learning process, when too many neurons are inert, that is, do not propagate any signals.\n",
    "\n",
    "The inputs and outputs will be binary. The function `binary_in` will turn all zeros into $Q$. The differential of `binary_in` is non-trivial, it is $=1$ on a $1$, and $=-1$ on a $0$, and then followed by a $\\text{ReLU}$. As an example:\n",
    "$$\n",
    "\\text{binary{\\_}in}_{\\text{backward at} [1, 0, 0, 1]}([y_0, y_1, y_2, y_3]) = \\text{ReLU}([y_0, -y_1, -y_2, y_3]). \n",
    "$$\n",
    "Is the $\\text{ReLU}$ a choice or required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c5a4c",
   "metadata": {},
   "source": [
    "#### Wrapping a float layer or model\n",
    "\n",
    "Let $L$ be a classical layer, model, or autograd function. We can wrap transform it to $L\\rightarrow L_{b}$ by composing with $H$ and $\\text{binary{\\_}in}$:\n",
    "$$\n",
    "L_{b} := H \\circ L \\circ \\text{binary{\\_}in}. \n",
    "$$\n",
    "This is not very intuitive, need an example here.\n",
    "\n",
    "Open points:\n",
    "* The initialization of parameters will likely be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d9cc8",
   "metadata": {},
   "source": [
    "#### Heuristic of the learning algorithm\n",
    "\n",
    "Let us consider the simplest case $y=L(x, w)$, in other words $x \\xrightarrow{L} y$, where $x, y$ are $1$-dimensional tensors. We will assume that $L$ has a backward pass given by differential analysis.\n",
    "\n",
    "We would like to understand the forward and backward pass of $L_b$:\n",
    "$$\n",
    "x \\xrightarrow{L_b} y, \\quad gy \\xrightarrow{L_{b, \\text{backward at\\,} x}} gx, gw.\n",
    "$$\n",
    "Now, $x$ and $y$ are binary and have values in $\\{0, 1\\}$. The gradients $gx, gy$ will be non-negative floats (thanks to the `ReLU` in `binary_in`), but let's ignore this. The gradient $gw$ will consist of floats. \n",
    "\n",
    "The significance of $gy$ is given by the following. Whenever $gy$ is positive, the behavior of $y$ should change: if it was $0$ then it should become $1$ and vice versa. The more positive $gy$ is, the more urgent it is for $y$ to change. If $gy$ is negative, then the behavior should stay the same.\n",
    "\n",
    "We compute\n",
    "$$\n",
    "gw = \\text{sign}(y-0.5) \\cdot gy\\cdot  L_{\\text{$w$-backward at\\,} \\text{binary{\\_}in}(x)}(1),\n",
    "$$\n",
    "where $\\text{sign}(y-0.5)$ is $=1$ if $y=1$ and $-1$ otherwise, it comes from the backward pass of the Heaviside. \n",
    "\n",
    "A modification $w - \\epsilon \\cdot gw$ would lead to \n",
    "$$\n",
    "L(\\text{binary{\\_}in}(x), w- \\epsilon \\cdot gw) = L(\\text{binary{\\_}in}(x), w) - \\epsilon \\cdot \\text{sign}(y-0.5) \\cdot gy \\cdot   (v, v),  \n",
    "$$\n",
    "where $v=L_{w-\\text{backward at\\,} \\text{binary{\\_}in}(x)}(1)$, and ignoring $\\epsilon^2$. If $y=1$ this is trying to change the value of $L$ according to the sign of $-gy$. If $y=0$, it is trying to change the value of $L$ according to the sign of $gy$. That's the desired behavior.\n",
    "\n",
    "Frequently, we will have an $L$, where $L_{w-\\text{backward at\\,} x}(1)=0$ if $x=0$. For example, that's the case if $L$ is linear. Therefore we use `binary_in` and get $L_{w-\\text{backward at\\,} \\text{binary{\\_}in}(x)}(1)\\neq 0$ in most cases.\n",
    "\n",
    "Let's consider $gx$. We compute\n",
    "$$\n",
    "gx = \\text{sign}(y-0.5)\\cdot \\text{sign}(x-0.5) \\cdot gy\\cdot L_{x-\\text{backward at\\,} x}(1).\n",
    "$$\n",
    "A modification $x - \\epsilon \\cdot gx$ doesn't really make sense for two reasons:\n",
    "* $x$ is binary\n",
    "* From the previous part, we know that the value of $x$ will change according to $-\\text{sign}(x-0.5) \\cdot gx$.\n",
    "\n",
    "For the first point, let's imagine $x=H(x')$. Then, by the second point, we should imagine a modification $x'- \\epsilon\\cdot \\text{sign}(x') \\cdot gx\\cdot (\\text{some non-negative value})$. This can only make a difference when $x'$ is very close to $0$ and $gx>0$.\n",
    "Therefore, we have  \n",
    "$$\n",
    "\\text{binary{\\_}in}(x_{\\text{modified}}) = \\begin{cases} x + (1-Q)\\cdot (-1)\\cdot \\text{sign}(x')  &\\text{if modified} \\\\ x &\\text{if no modification occurs.} \\end{cases}\n",
    "$$  \n",
    "In case a proper modification occurs, we compute\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\text{binary{\\_}in}(x_{\\text{modified}}), w) = L(\\text{binary{\\_}in}(x), w)  &+  (1-Q)\\cdot (-1)\\cdot \\text{sign}(x-0.5) \\cdot  L_{x-\\text{backward at\\,} x}(1) \\\\ &+ (\\text{higher order terms}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "At this point we have to make a simplification. The higher order terms cannot be controlled if we have higher derivatives for $L$ in $x$. That is, the backward pass $L_{x-\\text{backward at\\,} x}$ must have vanishing $x$-derivative. This is a heavy restriction.\n",
    "\n",
    "Then \n",
    "$$\n",
    "(1-Q)\\cdot (-1)\\cdot \\text{sign}(x-0.5) \\cdot  L_{x-\\text{backward at\\,} x}(1) = (1-Q)\\cdot (-1) \\cdot \\text{sign}(y-0.5) \\cdot \\frac{gx}{gy},\n",
    "$$\n",
    "where $gy\\neq 0$ and $gx>0$, because otherwise no modification. This shows that $L(\\text{binary{\\_}in}(x_{\\text{modified}}), w)$ is modified (if at all) according to the sign: $-\\text{sign}(y-0.5)\\cdot gy$, which is the desired behavior.  \n",
    "\n",
    "TODO: Arbitrary dimension for $x, y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34711bd0",
   "metadata": {},
   "source": [
    "#### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a485b3ff",
   "metadata": {},
   "source": [
    "##### Oja's rule\n",
    "(This doesn't seem to work well)\n",
    "\n",
    "Setup: $x \\xrightarrow{w} y$, we don't have a gradient for $y$ or $x$, but one for $w$:\n",
    "$$\n",
    "gw[i,j] := y[i]\\cdot (y[i]\\cdot w[i,j] - x[j]\\cdot B). \n",
    "$$\n",
    "(Or some other positive multiple of $B$.)\n",
    "\n",
    "We have only non-zero gradients if $y[i]=1$. Additionally, we get a *positive* gradient if \n",
    "* $x[j]=1$ and $w[i,j]>B$, \n",
    "* or $x[j]=0$ and $w[i,j]>0$. \n",
    "\n",
    "We get a *negative* gradient if \n",
    "* $x[j]=1$ and $w[i,j]<B$, \n",
    "* or $x[j]=0$ and $w[i,j]<0$.\n",
    "\n",
    "This implies two behaviours:\n",
    "* If $x[j]$ is inert then the weight will converge to zero.\n",
    "* The weight will be bounded by $B$ or some other positive multiple of $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd70ae72",
   "metadata": {},
   "source": [
    "##### Generalized Hebbian algorithm\n",
    "\n",
    "Setup: $x \\xrightarrow{w} y$, we don't have a gradient for $y$ or $x$, but one for $w$:\n",
    "$$\n",
    "gw[i,j] := y[i]\\cdot (- x[j]\\cdot B + \\sum_{k} y[k]\\cdot w[k,j]). \n",
    "$$\n",
    "(Or some other to be determined positive multiple of $B$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2e1ad",
   "metadata": {},
   "source": [
    "### Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11805e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e70decd",
   "metadata": {},
   "source": [
    "#### Autograd functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0632c8f",
   "metadata": {},
   "source": [
    "##### Heaviside and ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46b93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([-1, 0, 1], dtype=torch.float32)\n",
    "value = torch.tensor([1], dtype=torch.float32)\n",
    "\n",
    "def H_forward(input):\n",
    "    return torch.heaviside(input, value)\n",
    "\n",
    "class HeavisideBN(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    # B and Q are added as tensors\n",
    "    def forward(ctx, input):\n",
    "        output = H_forward(input)\n",
    "        #print(\"In forward: {}\".format(output))\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, = ctx.saved_tensors\n",
    "\n",
    "        grad_input  = None\n",
    "        grad_input = grad_output * (2. * output - 1.)\n",
    "    \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8ec7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = HeavisideBN.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b82ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  1.,  1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_forward(input)==H(input)\n",
    "\n",
    "input.requires_grad = True\n",
    "\n",
    "H(input).backward(torch.tensor([1., 1., 1.]))\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585582c",
   "metadata": {},
   "source": [
    "##### binary_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8154e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU = torch.nn.ReLU()\n",
    "LReLU = torch.nn.LeakyReLU(negative_slope=0.5, inplace=False)\n",
    "\n",
    "class BinaryIn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    # Q is added as tensor\n",
    "    def forward(ctx, input, Q):\n",
    "        output = input + Q * (1.-input)\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "\n",
    "        grad_input  = None\n",
    "        grad_input = grad_output * (2. * input - 1.)\n",
    "    \n",
    "        return LReLU(grad_input), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea59fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_in = BinaryIn.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a1655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.0100, 0.0100, 1.0000], grad_fn=<BinaryInBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000, -0.5000, -0.5000,  1.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([1, 0, 0, 1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "t = binary_in(input, torch.tensor([0.01]))\n",
    "print(t)\n",
    "\n",
    "t.backward(torch.tensor([1., 1., 1., 1.]))\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e9f1f",
   "metadata": {},
   "source": [
    "##### Gradient correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d67bad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientActivation(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, factor: torch.Tensor):\n",
    "        ctx.save_for_backward(factor)\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        factor, = ctx.saved_tensors\n",
    "\n",
    "        grad_output = grad_output * factor\n",
    "\n",
    "        return grad_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "697df9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_activation = GradientActivation.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "340667ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 1.], grad_fn=<GradientActivationBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([1, 0, 0, 1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "t = gradient_activation(input, torch.tensor([2]))\n",
    "print(t)\n",
    "\n",
    "t.backward(torch.tensor([1., 1., 1., 1.]))\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa7e1b",
   "metadata": {},
   "source": [
    "#### Wrapping class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bebc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import architecture\n",
    "\n",
    "\n",
    "class BN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model, Q):\n",
    "        super(BN, self).__init__()\n",
    "        self.core = model\n",
    "        self.Q = Q\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = binary_in(input, self.Q)\n",
    "        x = self.core(x)\n",
    "        x = H(x)\n",
    "        return x\n",
    "\n",
    "def bn(arch, Q):\n",
    "    def new(*args, **kwargs):\n",
    "        model = arch(*args, **kwargs)\n",
    "        return BN(model, Q)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "193404c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = bn(torch.nn.Linear, torch.tensor([0.05]))(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c43197de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.], grad_fn=<HeavisideBNBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(torch.tensor([0.1, 2.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df4e68cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.5061, -0.4817],\n",
       "         [-0.6487,  0.4058]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.6605, -0.4389], requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t.core.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d58442",
   "metadata": {},
   "source": [
    "##### Linear model\n",
    "\n",
    "We will start with constant bias.\n",
    "\n",
    "TODO: \n",
    "* Fix scale parameter, it's unclear how to set a meaningful one. \n",
    "* Weight init does not work properly for large dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "534a5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(shape, scale, B):\n",
    "\n",
    "    first = scale * torch.rand(shape, dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "    # make sure the rows sum up to B\n",
    "    first += B * torch.tensor([1/shape[1]], dtype=torch.float32) \n",
    "\n",
    "    first.requires_grad = True\n",
    "    return first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abdcc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBN(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(LinearBN, self).__init__()\n",
    "        assert \"Q\" in kwargs, \"Pass Q (leaky zero) as kwarg\"\n",
    "        assert \"B\" in kwargs, \"Pass B (bias) as kwarg\"\n",
    "        Q = kwargs.pop(\"Q\")\n",
    "        B = kwargs.pop(\"B\")\n",
    "        self.main = bn(torch.nn.Linear, Q)(*args, **kwargs)\n",
    "        self.core = self.main.core\n",
    "        self.constant_bias(-B)\n",
    "        self.re_init(B)\n",
    "    \n",
    "    def re_init(self, B):\n",
    "        pass\n",
    "        # TODO correct scale and initialization?\n",
    "        # scale = 2*B\n",
    "        # with torch.no_grad():\n",
    "        #     self.core.weight = torch.nn.Parameter(weights_init((self.core.out_features, self.core.in_features), \n",
    "        #                                                         scale, B))\n",
    "                                                                \n",
    "    def constant_bias(self, B):\n",
    "        self.core.bias.requires_grad = False\n",
    "        with torch.no_grad():\n",
    "            self.core.bias.fill_(B)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c462a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = LinearBN(2,2, Q=torch.tensor([0.05]), B=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae7eac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.2775,  0.3221],\n",
       "         [ 0.0642, -0.0161]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1., -1.])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t.core.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d58442",
   "metadata": {},
   "source": [
    "##### Convolution layer\n",
    "\n",
    "We will start with constant bias.\n",
    "\n",
    "TODO\n",
    "* Weight init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abdcc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBN(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Conv2dBN, self).__init__()\n",
    "        assert \"Q\" in kwargs, \"Pass Q (leaky zero) as kwarg\"\n",
    "        assert \"B\" in kwargs, \"Pass B (bias) as kwarg\"\n",
    "        Q = kwargs.pop(\"Q\")\n",
    "        B = kwargs.pop(\"B\")\n",
    "        self.main = bn(torch.nn.Conv2d, Q)(*args, **kwargs)\n",
    "        self.core = self.main.core\n",
    "        self.constant_bias(-B)\n",
    "        self.re_init(B)\n",
    "    \n",
    "    def re_init(self, B):\n",
    "        pass\n",
    "                                                                \n",
    "    def constant_bias(self, B):\n",
    "        self.core.bias.requires_grad = False\n",
    "        with torch.no_grad():\n",
    "            self.core.bias.fill_(B)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2f496",
   "metadata": {},
   "source": [
    "##### Gradient correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e29c3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleGradient(torch.nn.Module):\n",
    "    def __init__(self, scale: torch.Tensor):\n",
    "        super(ScaleGradient, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        return gradient_activation(input, self.scale)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2660d",
   "metadata": {},
   "source": [
    "#### Model helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf06fbc",
   "metadata": {},
   "source": [
    "Statistics on the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ca3b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradStat():\n",
    "\n",
    "    def __init__(self, tensor):\n",
    "        self.value = 0.\n",
    "        self.tensor = tensor\n",
    "\n",
    "    def update(self):\n",
    "        with torch.no_grad():\n",
    "            self.value += torch.mean(torch.abs(self.tensor.grad)).item()\n",
    "\n",
    "    def clear(self):\n",
    "        self.value = 0.\n",
    "    \n",
    "    def mean(self, counts):\n",
    "        return self.value/counts\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc7fa2",
   "metadata": {},
   "source": [
    "#### Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef95abb",
   "metadata": {},
   "source": [
    "##### Neq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d89f96e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 1.]]), tensor([[0., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[0, 1, 1, 1]], dtype=torch.float32)\n",
    "s = torch.tensor([[0, 0, 0, 1]], dtype=torch.float32)\n",
    "s, t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f51f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neq(s,t):\n",
    "    return 1-(s==t).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21500d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neq(s,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef41f3c",
   "metadata": {},
   "source": [
    "#### Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c23fe8",
   "metadata": {},
   "source": [
    "##### Oja's rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ebd0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: y are the inputs, x the outputs\n",
    "\n",
    "def oja(x, y, w, B, epsilon=torch.tensor(0.001)):\n",
    "    x_reshape = torch.reshape(x, x.shape+(1,))\n",
    "    first_matrix = x_reshape * x_reshape * w\n",
    "    second_matrix =  (1 + epsilon) * B* torch.matmul(x_reshape, torch.reshape(y, (y.shape[0], 1) + y.shape[1:]))\n",
    "    return first_matrix-second_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63d622f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000],\n",
       "         [0.4500, 0.1315]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_oja = torch.tensor([[1.0276, 0.3],\n",
    "        [0.45, 1.1325]], requires_grad=True)\n",
    "x_oja = torch.tensor([[0., 1.]])\n",
    "y_oja = torch.tensor([[0., 1.]])\n",
    "B = torch.tensor([1.])\n",
    "\n",
    "oja(x_oja, y_oja, w_oja, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a0c69",
   "metadata": {},
   "source": [
    "##### Generalized Hebbian algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ebd0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: y are the inputs, x the outputs\n",
    "\n",
    "def hebbian(x, y, w, B, epsilon=torch.tensor(0.5)):\n",
    "    x_reshape = torch.reshape(x, x.shape+(1,))\n",
    "    vector = torch.matmul(x, w)\n",
    "    first_matrix = torch.matmul(x_reshape, torch.reshape(vector, (vector.shape[0], 1) + vector.shape[1:]))   \n",
    "    second_matrix =  (1 + epsilon) * B* torch.matmul(x_reshape, torch.reshape(y, (y.shape[0], 1) + y.shape[1:]))\n",
    "    return first_matrix-second_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e811769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000],\n",
       "         [ 0.4500, -0.3675]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_heb = torch.tensor([[1.0276, 0.3],\n",
    "        [0.45, 1.1325]], requires_grad=True)\n",
    "x_heb = torch.tensor([[0., 1.]])\n",
    "y_heb = torch.tensor([[0., 1.]])\n",
    "B = torch.tensor([1.])\n",
    "\n",
    "hebbian(x_heb, y_heb, w_heb, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c1282",
   "metadata": {},
   "source": [
    "#### Comparison with ReLUs\n",
    "\n",
    "In order to compare performance with a non binary approach, we introduce a simple linear layer with constant bias and ReLU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ceae815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_compare(y, weights, B):\n",
    "    l = torch.matmul(y, weights.t()) - B\n",
    "    return torch.nn.ReLU()(l)\n",
    "\n",
    "def loss_gradient(o, output_bn):\n",
    "    return output_bn-o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0c007",
   "metadata": {},
   "source": [
    "#### Learning tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96003491",
   "metadata": {},
   "source": [
    "##### Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4543e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_model(inputs, desired_outputs, model, learning_rate, \n",
    "            binarizatio_fn=lambda input: torch.heaviside(input, torch.tensor([0.])),\n",
    "            loss_gradient = neq, print_it=False, grad_stat=[]):\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    eq_counter = 0\n",
    "    for i, o in zip(inputs, desired_outputs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(i)\n",
    "        output_bn = binarizatio_fn(output)\n",
    "        \n",
    "        if print_it:\n",
    "            with torch.no_grad():\n",
    "                print(\"{} -> {}\".format(i.numpy(), output_bn.numpy()))\n",
    "\n",
    "        if torch.all(o==output_bn):\n",
    "            eq_counter += 1\n",
    "        \n",
    "        gradient = loss_gradient(o, output_bn)\n",
    "            \n",
    "        output.backward(gradient)\n",
    "        \n",
    "        if print_it:\n",
    "            for w in model.parameters():\n",
    "                print(\"Weight grads: {}\".format(w.grad))\n",
    "\n",
    "        if grad_stat:\n",
    "            for layer in grad_stat:\n",
    "                getattr(model, layer).stat.update()\n",
    "\n",
    "        optimizer.step()\n",
    "                    \n",
    "    if eq_counter == len(inputs):\n",
    "        return \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438fe0c",
   "metadata": {},
   "source": [
    "##### Unsupervised\n",
    "\n",
    "Place holder. Nothing here yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4543e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_unsupervised(inputs, model, learning_rate, print_it=False):\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for i in inputs:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(i)\n",
    "        \n",
    "        if print_it:\n",
    "            with torch.no_grad():\n",
    "                print(\"{} -> {}\".format(i.numpy(), output.numpy()))\n",
    "        \n",
    "        gradient = torch.zeros(output.shape, dtype=output.dtype)\n",
    "            \n",
    "        output.backward(gradient)\n",
    "        if print_it:\n",
    "            for w in model.parameters():\n",
    "                print(\"Weight grads: {}\".format(w.grad))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe65a9c",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "All examples start with random initialisation of weights in the range $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15325304",
   "metadata": {},
   "source": [
    "#### Copy one neuron\n",
    "\n",
    "Architecture: `input_neuron -> output_neuron` \n",
    "\n",
    "Desired behavior: `0->0, 1->1`\n",
    "\n",
    "Explicitly, for forward $y=H(x\\cdot w-B)$. If $x=0$ then $y=0$, which is desired behavior. Suppose $x=1$ and $y=0$. Desired value is $y'=1$. For backward: the first gradient is $gy= Eq(y', y)=1$. The weight gradient reads $gw = W(gy, y, x)= 1$, pointing to increasing $w$, which is correct behavior. \n",
    "\n",
    "Once correct behavior is established, the weight gradients are zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "202e1e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in epoch 8\n",
      "[1.] -> [1.]\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1.]], requires_grad=True)\n",
    "desired_outputs = torch.tensor([[1.]], requires_grad=True)\n",
    "\n",
    "model = LinearBN(1,1, Q=torch.tensor([0.05]), B=1.)\n",
    "epochs=20\n",
    "\n",
    "for i in range(epochs):\n",
    "        done = learn_model(inputs, desired_outputs, model, learning_rate=0.1)\n",
    "        if done == \"Done\":\n",
    "            print(\"Finished in epoch {}\".format(i))\n",
    "            break\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae87f7c",
   "metadata": {},
   "source": [
    "#### Copy one neuron with other neurons in between\n",
    "\n",
    "Architecture: `input_neuron -> other_neuron -> output_neuron` \n",
    "\n",
    "Desired behavior: `0->0, 1->1`\n",
    "\n",
    "The backpropagation of \n",
    "$$\n",
    "x_\\text{in} \\xrightarrow{w_\\text{in}} x_\\text{other} \\xrightarrow{w_\\text{other}} x_{\\text{out}}\n",
    "$$\n",
    "is computed as follows\n",
    "$$\n",
    "gx_\\text{out} \\xrightarrow{gw_\\text{other}} gx_\\text{other} \\xrightarrow{gw_\\text{in}} gx_\\text{in}$$\n",
    "Independent of the $x_{\\text{other}}$ values, $gw_\\text{other}$ will be positive if the desired value for $x_\\text{out}$ is not reached. Once $w_\\text{other}$ is large enough, $gx_\\text{other}$ will become positive if $x_\\text{out}=0$ is not desired and $x_{\\text{other}}=0$. This will push $gw_\\text{in}$ to positive, fixing $w_\\text{in}$. The model will solve the task.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30c7f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in epoch 609\n",
      "[1.] -> [1.]\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1.]], requires_grad=True)\n",
    "desired_outputs = torch.tensor([[1.]], requires_grad=True)\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(LinearBN(1,1,Q=torch.tensor([0.05]), B=1.),\n",
    "                             LinearBN(1,1,Q=torch.tensor([0.05]), B=1.),\n",
    "                             LinearBN(1,1,Q=torch.tensor([0.05]), B=1.),\n",
    "                             LinearBN(1,1,Q=torch.tensor([0.05]), B=1.),\n",
    "                             LinearBN(1,1,Q=torch.tensor([0.05]), B=1.))\n",
    "\n",
    "epochs=1000\n",
    "\n",
    "for i in range(epochs):\n",
    "        done = learn_model(inputs, desired_outputs, model, learning_rate=0.1)\n",
    "        if done == \"Done\":\n",
    "            print(\"Finished in epoch {}\".format(i))\n",
    "            break\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f1f37",
   "metadata": {},
   "source": [
    "#### Switch positions\n",
    "\n",
    "Architecture: `input_neuron_0, input_neuron_1  -> output_neuron_0, output_neuron_1` \n",
    "\n",
    "Desired behavior: `I:(1,0)->(0,1), II:(0,1)->(1,0), III:(1,1)->(1,1)`\n",
    "\n",
    "Explicitly, for forward $y_i=H(x_0\\cdot w_{i,0} + x_1\\cdot w_{i,1} - B)$. Let's consider the weights one by one:\n",
    "* $w_{0,0}$: since `I` and `III` send contradictory impulses at first, no changes.\n",
    "* $w_{0,1}$: increasing, so it will fix `II` and have a strong angle on `III`. This will allow $w_{0,0}$ to focus on `I` and fix it. \n",
    "* $w_{1,0}$: Same as $w_{0,1}$.\n",
    "* $w_{1,1}$: Same as $w_{0,0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3014401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in epoch 10\n",
      "[1. 0.] -> [0. 1.]\n",
      "[0. 1.] -> [1. 0.]\n",
      "[1. 1.] -> [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1., 0.], [0., 1.], [1., 1.]], requires_grad=True)\n",
    "desired_outputs = torch.tensor([[0, 1.], [1., 0.], [1., 1.]], requires_grad=True)\n",
    "\n",
    "model = LinearBN(2,2, Q=torch.tensor([0.05]), B=1.)\n",
    "\n",
    "epochs = 600\n",
    "for i in range(epochs):\n",
    "        done = learn_model(inputs, desired_outputs, model, learning_rate=0.1)\n",
    "        if done == \"Done\":\n",
    "            print(\"Finished in epoch {}\".format(i))\n",
    "            break\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94f3f7",
   "metadata": {},
   "source": [
    "#### Hidden representation\n",
    "\n",
    "Architecture: `input_0, input_1  -> hidden_0, hidden_1, hidden_2 -> output` \n",
    "\n",
    "Desired behavior: `I:(1,0)->(1), II:(0,1)->(1), III:(1,1)->(0)`\n",
    "\n",
    "This cannot be learned with one layer only, because `I` and `II` imply two weights that are larger than the bias. This contradicts `III`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed424cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in epoch 92\n",
      "[[1. 0.]] -> [[1.]]\n",
      "[[0. 1.]] -> [[1.]]\n",
      "[[1. 1.]] -> [[0.]]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import use\n",
    "\n",
    "\n",
    "inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True),\n",
    "          torch.tensor([[1., 1.]], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([[1.]]), torch.tensor([[1.]]), torch.tensor([[0.]])]\n",
    "\n",
    "use_Q = torch.tensor([0.1])\n",
    "\n",
    "model = torch.nn.Sequential(LinearBN(2,3, Q=use_Q, B=1.),\n",
    "                            LinearBN(3,1, Q=use_Q, B=1.))\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for i in range(epochs):\n",
    "    done = learn_model(inputs, desired_outputs, model, learning_rate=0.05)\n",
    "    if done == \"Done\":\n",
    "        print(\"Finished in epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c7025",
   "metadata": {},
   "source": [
    "#### Hidden representation 2\n",
    "\n",
    "Architecture: `input_0, input_1  -> hidden_0, hidden_1 -> output` \n",
    "\n",
    "Desired behavior: `I:(1,0)->(1), II:(0,1)->(1), III:(1,1)->(0)`\n",
    "\n",
    "This cannot be learned with one layer only, because `I` and `II` imply two weights that are larger than the bias. This contradicts `III`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91707287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in epoch 144\n",
      "[[1. 0.]] -> [[1.]]\n",
      "[[0. 1.]] -> [[1.]]\n",
      "[[1. 1.]] -> [[0.]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True),\n",
    "          torch.tensor([[1., 1.]], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([[1.]]), torch.tensor([1.]), torch.tensor([[0.]])]\n",
    "\n",
    "use_Q = torch.tensor([0.1])\n",
    "\n",
    "model = torch.nn.Sequential(LinearBN(2,2, Q=use_Q, B=1.),\n",
    "                            LinearBN(2,1, Q=use_Q, B=1.))\n",
    "\n",
    "epochs = 3000\n",
    "\n",
    "for i in range(epochs):\n",
    "    done = learn_model(inputs, desired_outputs, model, learning_rate=0.05)\n",
    "    if done == \"Done\":\n",
    "        print(\"Finished in epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c7025",
   "metadata": {},
   "source": [
    "#### Getting through layers\n",
    "\n",
    "Architecture: `input_0, input_1  -> layer_1 -> layer_2 -> ... -> layer_d` \n",
    "\n",
    "All layers are `2`-dimensional, and the depth `d` is variable. \n",
    "\n",
    "Desired behavior: `(1,0)->(1,0), (0,1)->(0,1)`\n",
    "\n",
    "Initial weights: random\n",
    "\n",
    "The task is to get through all the randomly initialized layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c44354ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def getting_through_layer_model_class(d=4, epochs=1000):\n",
    "    inputs = [torch.tensor([[1., 0.], [0., 1.]], requires_grad=True)]\n",
    "    desired_outputs = [torch.tensor([[1., 0.], [0., 1.]])]\n",
    "\n",
    "    layers = [('layer_{}'.format(i+1), LinearBN(2,2, Q=torch.tensor([0.05]), B=1.)) for i in range(d)]\n",
    "\n",
    "    model = torch.nn.Sequential(collections.OrderedDict(layers))\n",
    "\n",
    "    #print(\"Layers : {}\".format(list(model.named_children())))\n",
    "\n",
    "    print(\"Initial weights: {}\".format([l.core.weight for _, l in list(model.named_children())]))\n",
    "\n",
    "    for i in range(epochs):\n",
    "        done = learn_model(inputs, desired_outputs, model, learning_rate=0.1)\n",
    "        if done==\"Done\":\n",
    "            print(\"Done in epoch {}\".format(i))\n",
    "            break\n",
    "\n",
    "    print(\"New weights: {}\".format([l.core.weight for _, l in list(model.named_children())]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in inputs:\n",
    "            print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))\n",
    "\n",
    "#getting_through_layer_model_class()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c312a",
   "metadata": {},
   "source": [
    "#### MNIST with binary models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb15458",
   "metadata": {},
   "source": [
    "##### Classical approach according to pytorch tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5699db",
   "metadata": {},
   "source": [
    "Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b23a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarization\n",
    "binarize_images = transforms.Compose([transforms.ToTensor(), lambda x: x>0.5, lambda x: x.float()])\n",
    "#binarize_images = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='/home/andre/data', train=True, \n",
    "                                                download=True, transform=binarize_images)\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bffaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec8db32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img):\n",
    "    with torch.no_grad():\n",
    "        img = torchvision.utils.make_grid(img)     \n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray', vmin=0, vmax=1)\n",
    "        plt.show()\n",
    "\n",
    "imshow(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5f8c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        #self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.pool1 = nn.Conv2d(6,6,2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
    "        self.pool2 = nn.Conv2d(12,12,2, stride=2)\n",
    "        self.fc1 = nn.Linear(192, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = MNISTNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a1564",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "*Note*: Run training again if loss is not decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c843ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a364f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cbook import report_memory\n",
    "\n",
    "\n",
    "def mnist_classical_training():\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "        running_loss = 0.\n",
    "        running_grad_conv1 = 0.\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inspect_grad = torch.abs(net.conv1.weight.grad).sum().numpy()\n",
    "\n",
    "        # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_grad_conv1 += inspect_grad\n",
    "            report_period = 200\n",
    "            if i % report_period == report_period-1:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / report_period :.3f}')\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] grad_conv1_changes: {running_grad_conv1 / report_period:.3f}')\n",
    "                running_loss = 0.0\n",
    "                running_grad_conv1 = 0.\n",
    "            \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "#mnist_classical_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63c355c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(images):\n",
    "    with torch.no_grad():\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        print('Predicted: ', ' '.join(f'{trainset.classes[predicted[j]]:5s}||'\n",
    "                                        for j in range(4)))\n",
    "        imshow(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25aaae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  4 - four|| 4 - four|| 4 - four|| 4 - four||\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4fdd23",
   "metadata": {},
   "source": [
    "Accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d32ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root='/home/andre/data', train=False,\n",
    "                                       download=True, transform=binarize_images)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef538518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_classical_accuracy():\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct // total} %')\n",
    "\n",
    "# mnist_classical_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb15458",
   "metadata": {},
   "source": [
    "##### Binary approach to MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58f566",
   "metadata": {},
   "source": [
    "Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5f8c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_MNIST = 0.25\n",
    "\n",
    "class MNISTNetBN(nn.Module):\n",
    "    def __init__(self, Q=torch.tensor([0.001]), B=B_MNIST):\n",
    "        super().__init__()\n",
    "        \n",
    "        qb = {\"Q\": Q, \"B\": B}\n",
    "        self.conv1 = Conv2dBN(1, 24, 5, stride=2, **qb)\n",
    "        self.conv2 = Conv2dBN(1, 16, 4, stride=2, **qb)\n",
    "        self.fc = LinearBN(6160, 10, **qb)\n",
    "        \n",
    "        #self.scale3 = ScaleGradient(torch.tensor([0.75]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        first = self.conv1(x) \n",
    "        second = self.conv2(x)\n",
    "        x = torch.cat([torch.flatten(l, 1) for l in [first, second]], dim=1) \n",
    "        #x = self.scale3(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.reshape(x, x.shape[0:2])\n",
    "        return x\n",
    "\n",
    "\n",
    "net = MNISTNetBN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee4ef6",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7bf0b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 1, 7, 1])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def labels_to_desired(labels):\n",
    "    return torch.nn.functional.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "print(labels)\n",
    "print(labels_to_desired(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd9fcff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bn(grad):\n",
    "    with torch.no_grad():\n",
    "        d = float(grad.shape[0]) * float(grad.shape[1])\n",
    "        grad_flat = torch.flatten(grad)\n",
    "        s = grad_flat.sum()\n",
    "        return (s/d).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c961124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_grad(grad, outputs, penalty = 1.):\n",
    "    filter_not_one = (torch.sum(outputs, -1) != 1.).float()\n",
    "    penalty_tensor = (1. + penalty * filter_not_one)\n",
    "    return grad * torch.reshape(penalty_tensor, penalty_tensor.shape + (1,))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a364f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_BN_training(lr_MNIST = 0.001, penalty=0.):\n",
    "    \n",
    "    lr_running = lr_MNIST \n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr_MNIST)\n",
    "    num_lower_lr = 10\n",
    "\n",
    "    for epoch in range(100):  # loop over the dataset multiple times\n",
    "        previous_running_loss = 0.\n",
    "        running_loss = 0.\n",
    "        running_grad = [\"conv1\", \"conv2\", \"fc\"]\n",
    "\n",
    "        for layer in running_grad:\n",
    "            getattr(net, layer).stat = GradStat(getattr(net, layer).core.weight)\n",
    "\n",
    "        if num_lower_lr==0:\n",
    "            break\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            one_hot_labels = labels_to_desired(labels)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                grad = neq(outputs, one_hot_labels)\n",
    "                grad_loss = improve_grad(grad, outputs, penalty=penalty)\n",
    "            \n",
    "            outputs.backward(grad_loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Debug\n",
    "            with torch.no_grad():\n",
    "                for layer in running_grad:\n",
    "                    getattr(net, layer).stat.update()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "            running_loss += loss_bn(grad_loss)     \n",
    "            report_period = 4000\n",
    "            if i % report_period == report_period-1:    \n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / report_period :.3f}')\n",
    "                for layer in running_grad:\n",
    "                    print(f'[{epoch + 1}, {i + 1:5d}] stats_{layer}_changes: {getattr(net, layer).stat.mean(2000) :.4f}')\n",
    "                \n",
    "                #reset\n",
    "                if running_loss>previous_running_loss + 0.001 and previous_running_loss!=0.:\n",
    "                    lr_running = 0.5 * lr_running\n",
    "                    optimizer = torch.optim.SGD(net.parameters(), lr=lr_running)\n",
    "                    num_lower_lr -= 1\n",
    "                    if num_lower_lr == 0:\n",
    "                        break\n",
    "                    print(\"Lowered learning rate\")\n",
    "                    \n",
    "                previous_running_loss = running_loss\n",
    "                running_loss = 0.\n",
    "                for layer in running_grad:\n",
    "                    getattr(net, layer).stat.clear()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "#mnist_BN_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005be3d",
   "metadata": {},
   "source": [
    "Accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0910ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_correct(t, labels):\n",
    "    filter_only_one = torch.sum(t, -1)==1\n",
    "    filter_one_and_correct = torch.argmax(t[filter_only_one], -1) == torch.argmax(labels_to_desired(labels)[filter_only_one], -1)\n",
    "    correct = filter_one_and_correct.sum().item()\n",
    "    false = filter_only_one.sum().item() - correct\n",
    "    return correct, false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef538518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_bn_statistics():\n",
    "    total_stats = {k: 0 for k in range(10)}\n",
    "    stat = {k: 0 for k in range(11)}\n",
    "    one_and_correct = {\"correct\": 0, \"false\": 0}\n",
    "    missed_by_no_prediction  = {k: 0 for k in range(10)}\n",
    "    one_detailed = {k: {\"correct\": 0, \"false\": 0} for k in range(10)}\n",
    "    confusion_matrix = np.zeros((10, 10), dtype=np.int16)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            for k in stat:\n",
    "                filter_k = torch.sum(outputs, -1)==k\n",
    "                stat[k] += (filter_k).sum().item()\n",
    "                \n",
    "                if k==0:\n",
    "                    for label in missed_by_no_prediction:\n",
    "                        missed_by_no_prediction[label] += (labels[filter_k]==label).sum().item()\n",
    "\n",
    "                if k==1:\n",
    "                    for prediction in one_detailed:\n",
    "                        outputs_one = outputs[filter_k]\n",
    "                        labels_one = labels[filter_k]\n",
    "                        filter_prediction = torch.argmax(outputs_one, -1) == prediction\n",
    "                        labels_prediction = labels_one[filter_prediction]\n",
    "\n",
    "                        num_results_label = num_correct(outputs_one[filter_prediction], labels_prediction)\n",
    "                        one_detailed[prediction][\"correct\"] += num_results_label[0]\n",
    "                        one_detailed[prediction][\"false\"] += num_results_label[1]\n",
    "                        \n",
    "                        for label in range(10):\n",
    "                            confusion_matrix[prediction, label] += (labels_prediction==label).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "            for k in total_stats:     \n",
    "                total_stats[k] += (labels == k).sum().item()\n",
    "\n",
    "            num_results = num_correct(outputs, labels)\n",
    "            one_and_correct[\"correct\"] += num_results[0]\n",
    "            one_and_correct[\"false\"] += num_results[1]\n",
    "\n",
    "    print(\"Total counts for the labels 0,1,..,9: {}\\n\".format(total_stats))\n",
    "    print(\"Number of predictions returned per input (1 is best, 0 means no prediction): {}\\n\".format(stat)) \n",
    "    print(\"Analysis when only one prediction is returned: {}\\n\".format(one_and_correct))\n",
    "    print(f'Accuracy on only one prediction: {one_and_correct[\"correct\"]/(stat[1]):.2f}')\n",
    "    print(\"Analysis when only one prediction is returned per label: {}\\n\".format(one_detailed))\n",
    "    print(\"For which labels 0,1,..,9 did the model not predict: {}\\n\".format(missed_by_no_prediction))\n",
    "    print(\"Confusion matrix\")\n",
    "    print(confusion_matrix)\n",
    "\n",
    "#mnist_bn_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfd192",
   "metadata": {},
   "source": [
    "Dominant weights\n",
    "\n",
    "Which are the most important nodes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 1.12 Kernel",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "96bacbb292411d9e8d48d6a003550589e7a6b28e03eb0e879664ddbf5f968d66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
