{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e12942",
   "metadata": {},
   "source": [
    "## Leaky binary models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bf12a",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "* Neuron values are either 0 or 1 (binary). The inputs and outputs for sure, and internally as much as possible.\n",
    "* Weights are real values, which are maybe bounded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c404ca3",
   "metadata": {},
   "source": [
    "#### Modified Heaviside\n",
    "\n",
    "The Heaviside function is defined as \n",
    "$$\n",
    "H(x):=\\begin{cases}1 \\quad x\\geq 0\\\\0 \\quad x< 0.\\end{cases}\n",
    "$$\n",
    "Mathematically, it has vanishing differential, because $H$ is essentially constant. \n",
    "\n",
    "<!-- We will use the following leaky variant of $H$, which depends on a non-negative constant $Q$:\n",
    "$$\n",
    "H(x, Q):= \\begin{cases} 1 \\quad x\\geq 0\\\\ Q \\quad x< 0.\\end{cases}\n",
    "$$ -->\n",
    "For our purpose, it will be convenient to use the following differential for $H$, \n",
    "$$\n",
    "H_{\\text{backward}} := \\begin{cases} 1 \\quad x\\geq 0\\\\ -1 \\quad x< 0.\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77a818",
   "metadata": {},
   "source": [
    "#### Internal binary representation\n",
    "\n",
    "In order to use classical float based layers for binary computations, we will use a leaky variant of a binary tensor. Instead of $[1, 0, 0, 1]$ we will use $[1, Q, Q, 1]$, where $Q<1$ is a non-negative constant, which is typically small. This will help to overcome a stop of the learning process, when too many neurons are inert, that is, do not propagate any signals.\n",
    "\n",
    "The inputs and outputs will be binary. The function `binary_in` will turn all zeros into $Q$. The differential of `binary_in` is non-trivial, it is $=1$ on a $1$, and $=-1$ on a $0$, and then followed by a $\\text{ReLU}$. As an example:\n",
    "$$\n",
    "\\text{binary{\\_}in}_{\\text{backward at} [1, 0, 0, 1]}([y_0, y_1, y_2, y_3]) = \\text{ReLU}([y_0, -y_1, -y_2, y_3]). \n",
    "$$\n",
    "Is the $\\text{ReLU}$ a choice or required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c5a4c",
   "metadata": {},
   "source": [
    "#### Wrapping a float layer or model\n",
    "\n",
    "Let $L$ be a classical layer, model, or autograd function. We can wrap transform it to $L\\rightarrow L_{b}$ by composing with $H$ and $\\text{binary{\\_}in}$:\n",
    "$$\n",
    "L_{b} := H \\circ L \\circ \\text{binary{\\_}in}. \n",
    "$$\n",
    "This is not very intuitive, need an example here.\n",
    "\n",
    "Open points:\n",
    "* The initialization of parameters will likely be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d9cc8",
   "metadata": {},
   "source": [
    "#### Heuristic of the learning algorithm\n",
    "\n",
    "Let us consider the simplest case $y=L(x, w)$, in other words $x \\xrightarrow{L} y$, where $x, y$ are $1$-dimensional tensors. We will assume that $L$ has a backward pass given by differential analysis.\n",
    "\n",
    "We would like to understand the forward and backward pass of $L_b$:\n",
    "$$\n",
    "x \\xrightarrow{L_b} y, \\quad gy \\xrightarrow{L_{b, \\text{backward at\\,} x}} gx, gw.\n",
    "$$\n",
    "Now, $x$ and $y$ are binary and have values in $\\{0, 1\\}$. The gradients $gx, gy$ will be non-negative floats (thanks to the `ReLU` in `binary_in`), but let's ignore this. The gradient $gw$ will consist of floats. \n",
    "\n",
    "The significance of $gy$ is given by the following. Whenever $gy$ is positive, the behavior of $y$ should change: if it was $0$ then it should become $1$ and vice versa. The more positive $gy$ is, the more urgent it is for $y$ to change. If $gy$ is negative, then the behavior should stay the same.\n",
    "\n",
    "We compute\n",
    "$$\n",
    "gw = \\text{sign}(y-0.5) \\cdot gy\\cdot  L_{\\text{$w$-backward at\\,} \\text{binary{\\_}in}(x)}(1),\n",
    "$$\n",
    "where $\\text{sign}(y-0.5)$ is $=1$ if $y=1$ and $-1$ otherwise, it comes from the backward pass of the Heaviside. \n",
    "\n",
    "A modification $w - \\epsilon \\cdot gw$ would lead to \n",
    "$$\n",
    "L(\\text{binary{\\_}in}(x), w- \\epsilon \\cdot gw) = L(\\text{binary{\\_}in}(x), w) - \\epsilon \\cdot \\text{sign}(y-0.5) \\cdot gy \\cdot   (v, v),  \n",
    "$$\n",
    "where $v=L_{w-\\text{backward at\\,} \\text{binary{\\_}in}(x)}(1)$, and ignoring $\\epsilon^2$. If $y=1$ this is trying to change the value of $L$ according to the sign of $-gy$. If $y=0$, it is trying to change the value of $L$ according to the sign of $gy$. That's the desired behavior.\n",
    "\n",
    "Frequently, we will have an $L$, where $L_{w-\\text{backward at\\,} x}(1)=0$ if $x=0$. For example, that's the case if $L$ is linear. Therefore we use `binary_in` and get $L_{w-\\text{backward at\\,} \\text{binary{\\_}in}(x)}(1)\\neq 0$ in most cases.\n",
    "\n",
    "Let's consider $gx$. We compute\n",
    "$$\n",
    "gx = \\text{sign}(y-0.5)\\cdot \\text{sign}(x-0.5) \\cdot gy\\cdot L_{x-\\text{backward at\\,} x}(1).\n",
    "$$\n",
    "A modification $x - \\epsilon \\cdot gx$ doesn't really make sense for two reasons:\n",
    "* $x$ is binary\n",
    "* From the previous part, we know that the value of $x$ will change according to $-\\text{sign}(x-0.5) \\cdot gx$.\n",
    "\n",
    "For the first point, let's imagine $x=H(x')$. Then, by the second point, we should imagine a modification $x'- \\epsilon\\cdot \\text{sign}(x') \\cdot gx\\cdot (\\text{some non-negative value})$. This can only make a difference when $x'$ is very close to $0$ and $gx>0$.\n",
    "Therefore, we have  \n",
    "$$\n",
    "\\text{binary{\\_}in}(x_{\\text{modified}}) = \\begin{cases} x + (1-Q)\\cdot (-1)\\cdot \\text{sign}(x')  &\\text{if modified} \\\\ x &\\text{if no modification occurs.} \\end{cases}\n",
    "$$  \n",
    "In case a proper modification occurs, we compute\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\text{binary{\\_}in}(x_{\\text{modified}}), w) = L(\\text{binary{\\_}in}(x), w)  &+  (1-Q)\\cdot (-1)\\cdot \\text{sign}(x-0.5) \\cdot  L_{x-\\text{backward at\\,} x}(1) \\\\ &+ (\\text{higher order terms}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "At this point we have to make a simplification. The higher order terms cannot be controlled if we have higher derivatives for $L$ in $x$. That is, the backward pass $L_{x-\\text{backward at\\,} x}$ must have vanishing $x$-derivative. This is a heavy restriction.\n",
    "\n",
    "Then \n",
    "$$\n",
    "(1-Q)\\cdot (-1)\\cdot \\text{sign}(x-0.5) \\cdot  L_{x-\\text{backward at\\,} x}(1) = (1-Q)\\cdot (-1) \\cdot \\text{sign}(y-0.5) \\cdot \\frac{gx}{gy},\n",
    "$$\n",
    "where $gy\\neq 0$ and $gx>0$, because otherwise no modification. This shows that $L(\\text{binary{\\_}in}(x_{\\text{modified}}), w)$ is modified (if at all) according to the sign: $-\\text{sign}(y-0.5)\\cdot gy$, which is the desired behavior.  \n",
    "\n",
    "TODO: Arbitrary dimension for $x, y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e813ea4c",
   "metadata": {},
   "source": [
    "### Linear model overview (old) \n",
    "\n",
    "Old approach, included for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde6a4b",
   "metadata": {},
   "source": [
    "#### Assumptions\n",
    "* Fixed bias to fix ideas (subject to change).\n",
    "\n",
    "#### Architecture\n",
    "Let $H(x)$ be the binary step function \n",
    "$$\n",
    "H(x):=\\begin{cases}1 \\quad x\\geq 0\\\\0 \\quad x< 0.\\end{cases}\n",
    "$$\n",
    "\n",
    "The *forward pass* of one layer reads \n",
    "$$\n",
    "L_i(x,w) := H\\left( \\sum_{j}  w[i,j]\\cdot x[j] - B \\right),\n",
    "$$\n",
    "where $x$ are the inputs, or neuron values, and $w$ are the weights, that is the synaptic values. We denote by $B$ the bias, it is a positive constant.\n",
    "\n",
    "From inputs to final outputs with $n$ layers, the forward pass looks like \n",
    "$$\n",
    "x_0 \\xrightarrow{w_0} x_1 \\xrightarrow{w_1} x_2 \\rightarrow \\dots \\rightarrow x_{n-1} \\xrightarrow{w_{n-1}} x_n,\n",
    "$$\n",
    "with \n",
    "$$\n",
    "x_i = L(x_{i-1}, w_{i-1}), \\quad x_0 = \\text{given inputs}.\n",
    "$$\n",
    "Again, $x$ has only entries in $\\{0, 1\\}$, and $w$ has floats as entries (bounded?).  \n",
    "\n",
    "The *backward pass* is given by\n",
    "$$\n",
    "gx_n \\xrightarrow{gw_{n-1}} gx_{n-1} \\xrightarrow{gw_{n-2}} gx_{n-2} \\rightarrow \\dots \\rightarrow gx_{1} \\xrightarrow{gw_{0}} gx_0,\n",
    "$$\n",
    "where \n",
    "\\begin{split}\n",
    "gx_i &= G(gx_{i+1}, x_{i+1}, x_{i}, w_{i})\\\\\n",
    "gw_i &= W(gx_{i+1}, x_{i+1}, x_i) \\\\\n",
    "gx_n &= Neq(\\text{desired outputs}, x_n).\n",
    "\\end{split}\n",
    "We will define $G$, $W$, and $Neq$ in the following. \n",
    "\n",
    "As for the forward pass, $gx$ has non-negative floats as entries, and $gw$ has floats as entries. Moreover, the shape of $x_i$ equals the shape of $gx_i$, and the shape of $w_i$ equals the shape of $gw_i$.\n",
    "\n",
    "#### Backward pass formulas\n",
    "\n",
    "##### Neq\n",
    "For $x, y \\in \\{0, 1\\}$, we set\n",
    "$$\n",
    "Neq(x, y) = \\begin{cases} 0 \\quad  &x=y, \\\\ 1 \\quad &x\\neq y. \\end{cases} \n",
    "$$\n",
    "This is generalized to $x$ and $y$ of the same shape by applying entry by entry. The signature is\n",
    "```\n",
    "Neq: (bool, shape) x (bool, shape) -> (bool, shape)  \n",
    "```\n",
    "\n",
    "##### W\n",
    "Let $n$ be the shape of $x$ and $gx$, and let $m$ be the shape of $x'$. We define $W(gx, x, x')$, which is of shape $n\\times m$, by \n",
    "$$\n",
    "W(gx, x, x') := \\left( -gx[i] \\cdot P(x[i], x'[j]) \\right)_{i,j},\n",
    "$$\n",
    "where \n",
    "$$\n",
    "P(0, 1) = 1, \\quad P(1, 1) = -1, \\quad P(0, 0) = Q, \\quad P(1, 0) = -Q,  \n",
    "$$\n",
    "with a non-negative $Q<1$. The value of $Q$ is connected to the learning rate (reference???). It is similar to a negative slope for a leaky ReLU. \n",
    "\n",
    "In order to compute $P$, we can use \n",
    "$$\n",
    "P(x,y) =  (1 - 2 \\cdot x) \\cdot (Q  + (1 - Q) \\cdot y) \n",
    "$$ \n",
    "\n",
    "The signature is\n",
    "```\n",
    "W: (bool, n) x (bool, n) x (bool, m) -> ({-1, -Q, 0, Q, 1}, n x m).  \n",
    "```\n",
    "\n",
    "##### G\n",
    "Let $n$ be the shape of $x$ and $gx$, and $m$ the shape of $x'$, then $w'$ must have shape $n\\times m$. We define \n",
    "$$\n",
    "G(gx, x, x', w') = \\left(ReLU\\left( \\sum_{i} gx[i]\\cdot Seq(x[i], x'[j])  \\cdot w[i,j] \\right) \\right)_{j},\n",
    "$$\n",
    "where \n",
    "$$\n",
    "Seq(x,y) = (2x-1)\\cdot (2y-1)  = \\begin{cases} 1 \\quad  &x=y, \\\\ -1 \\quad &x\\neq y. \\end{cases}\n",
    "$$\n",
    "The signature is \n",
    "```\n",
    "G: (bool, n) x (bool, n) x (bool, m) x (float, n x m) -> (non-negative, m).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2a866",
   "metadata": {},
   "source": [
    "#### Unsupervised methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a485b3ff",
   "metadata": {},
   "source": [
    "##### Oja's rule\n",
    "(This doesn't seem to work well)\n",
    "\n",
    "Setup: $x \\xrightarrow{w} y$, we don't have a gradient for $y$ or $x$, but one for $w$:\n",
    "$$\n",
    "gw[i,j] := y[i]\\cdot (y[i]\\cdot w[i,j] - x[j]\\cdot B). \n",
    "$$\n",
    "(Or some other positive multiple of $B$.)\n",
    "\n",
    "We have only non-zero gradients if $y[i]=1$. Additionally, we get a *positive* gradient if \n",
    "* $x[j]=1$ and $w[i,j]>B$, \n",
    "* or $x[j]=0$ and $w[i,j]>0$. \n",
    "\n",
    "We get a *negative* gradient if \n",
    "* $x[j]=1$ and $w[i,j]<B$, \n",
    "* or $x[j]=0$ and $w[i,j]<0$.\n",
    "\n",
    "This implies two behaviours:\n",
    "* If $x[j]$ is inert then the weight will converge to zero.\n",
    "* The weight will be bounded by $B$ or some other positive multiple of $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd70ae72",
   "metadata": {},
   "source": [
    "##### Generalized Hebbian algorithm\n",
    "\n",
    "Setup: $x \\xrightarrow{w} y$, we don't have a gradient for $y$ or $x$, but one for $w$:\n",
    "$$\n",
    "gw[i,j] := y[i]\\cdot (- x[j]\\cdot B + \\sum_{k} y[k]\\cdot w[k,j]). \n",
    "$$\n",
    "(Or some other to be determined positive multiple of $B$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2e1ad",
   "metadata": {},
   "source": [
    "### Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11805e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0632c8f",
   "metadata": {},
   "source": [
    "#### Heaviside and ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46b93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([-1, 0, 1], dtype=torch.float32)\n",
    "value = torch.tensor([1], dtype=torch.float32)\n",
    "\n",
    "def H_forward(input):\n",
    "    return torch.heaviside(input, value)\n",
    "\n",
    "class HeavisideBN(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    # B and Q are added as tensors\n",
    "    def forward(ctx, input):\n",
    "        output = H_forward(input)\n",
    "        #print(\"In forward: {}\".format(output))\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, = ctx.saved_tensors\n",
    "\n",
    "        grad_input  = None\n",
    "        grad_input = grad_output * (2. * output - 1.)\n",
    "    \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8ec7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = HeavisideBN.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b82ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  1.,  1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_forward(input)==H(input)\n",
    "\n",
    "input.requires_grad = True\n",
    "\n",
    "H(input).backward(torch.tensor([1., 1., 1.]))\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585582c",
   "metadata": {},
   "source": [
    "#### binary_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8154e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLU = torch.nn.ReLU()\n",
    "LReLU = torch.nn.LeakyReLU(negative_slope=0.5, inplace=False)\n",
    "\n",
    "class BinaryIn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    # B and Q are added as tensors\n",
    "    def forward(ctx, input, Q):\n",
    "        output = input + Q * (1.-input)\n",
    "        #print(\"In forward: {}\".format(output))\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "\n",
    "        grad_input  = None\n",
    "        grad_input = grad_output * (2. * input - 1.)\n",
    "    \n",
    "        return LReLU(grad_input), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea59fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_in = BinaryIn.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a1655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.0100, 0.0100, 1.0000], grad_fn=<BinaryInBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000, -0.5000, -0.5000,  1.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([1, 0, 0, 1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "t = binary_in(input, torch.tensor([0.01]))\n",
    "print(t)\n",
    "\n",
    "t.backward(torch.tensor([1., 1., 1., 1.]))\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa7e1b",
   "metadata": {},
   "source": [
    "#### Wrapping class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bebc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import architecture\n",
    "\n",
    "\n",
    "class BN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model, Q):\n",
    "        super(BN, self).__init__()\n",
    "        self.core = model\n",
    "        self.Q = Q\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = binary_in(input, self.Q)\n",
    "        x = self.core(x)\n",
    "        x = H(x)\n",
    "        return x\n",
    "\n",
    "def bn(arch, Q):\n",
    "    def new(*args, **kwargs):\n",
    "        model = arch(*args, **kwargs)\n",
    "        return BN(model, Q)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "193404c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = bn(torch.nn.Linear, torch.tensor([0.05]))(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43197de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.], grad_fn=<HeavisideBNBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(torch.tensor([0.1, 2.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df4e68cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1065, -0.5896],\n",
       "         [ 0.1417,  0.0650]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.2343, -0.3992], requires_grad=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t.core.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d58442",
   "metadata": {},
   "source": [
    "##### Linear model\n",
    "\n",
    "We will start with constant bias.\n",
    "\n",
    "TODO: \n",
    "* Fix scale parameter, it's unclear how to set a meaningful one. \n",
    "* Weight init does not work properly for large dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "534a5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(shape, scale, B):\n",
    "\n",
    "    first = scale * torch.rand(shape, dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "    # make sure the rows sum up to B\n",
    "    first += B * torch.tensor([1/shape[1]], dtype=torch.float32) \n",
    "\n",
    "    first.requires_grad = True\n",
    "    return first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abdcc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearB(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(LinearB, self).__init__()\n",
    "        assert \"Q\" in kwargs, \"Pass Q (leaky zero) as kwarg\"\n",
    "        assert \"B\" in kwargs, \"Pass B (bias) as kwarg\"\n",
    "        Q = kwargs.pop(\"Q\")\n",
    "        B = kwargs.pop(\"B\")\n",
    "        self.main = bn(torch.nn.Linear, Q)(*args, **kwargs)\n",
    "        self.core = self.main.core\n",
    "        self.constant_bias(-B)\n",
    "        self.re_init(B)\n",
    "    \n",
    "    def re_init(self, B):\n",
    "        pass\n",
    "        # TODO correct scale and initialization?\n",
    "        # scale = 2*B\n",
    "        # with torch.no_grad():\n",
    "        #     self.core.weight = torch.nn.Parameter(weights_init((self.core.out_features, self.core.in_features), \n",
    "        #                                                         scale, B))\n",
    "                                                                \n",
    "    def constant_bias(self, B):\n",
    "        self.core.bias.requires_grad = False\n",
    "        with torch.no_grad():\n",
    "            self.core.bias.fill_(B)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c462a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = LinearB(2,2, Q=torch.tensor([0.05]), B=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae7eac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.2269, 0.1877],\n",
       "         [0.0917, 0.5701]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1., -1.])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t.core.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d58442",
   "metadata": {},
   "source": [
    "##### Convolution layer\n",
    "\n",
    "We will start with constant bias.\n",
    "\n",
    "TODO: \n",
    "* Weight init does not work properly for large dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abdcc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBN(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Conv2dBN, self).__init__()\n",
    "        assert \"Q\" in kwargs, \"Pass Q (leaky zero) as kwarg\"\n",
    "        assert \"B\" in kwargs, \"Pass B (bias) as kwarg\"\n",
    "        Q = kwargs.pop(\"Q\")\n",
    "        B = kwargs.pop(\"B\")\n",
    "        self.main = bn(torch.nn.Conv2d, Q)(*args, **kwargs)\n",
    "        self.core = self.main.core\n",
    "        self.constant_bias(-B)\n",
    "        self.re_init(B)\n",
    "    \n",
    "    def re_init(self, B):\n",
    "        pass\n",
    "                                                                \n",
    "    def constant_bias(self, B):\n",
    "        self.core.bias.requires_grad = False\n",
    "        with torch.no_grad():\n",
    "            self.core.bias.fill_(B)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b9c9d",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a944572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2000,  0.1000,  1.1000],\n",
       "         [ 0.0000, -0.4000,  0.3000]], requires_grad=True),\n",
       " tensor([[0., 1., 1.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([[0.2, 0.1, 1.1], [0, -0.4, 0.3]], dtype=torch.float32, requires_grad=True)\n",
    "x = torch.tensor([[0, 1, 1]], dtype=torch.float32)\n",
    "weights, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e972dc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2000, -0.1000]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x, weights.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d3aa5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_bool(weights, input, B):\n",
    "    return H(torch.matmul(input, weights.t()) - B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a441959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.]], grad_fn=<HeavisideBNBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_bool(weights, x, torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc7fa2",
   "metadata": {},
   "source": [
    "#### Neq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d89f96e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 1.]]), tensor([[0., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[0, 1, 1, 1]], dtype=torch.float32)\n",
    "s = torch.tensor([[0, 0, 0, 1]], dtype=torch.float32)\n",
    "s, t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f51f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neq(s,t):\n",
    "    return 1-(s==t).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21500d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neq(s,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48ed2a1",
   "metadata": {},
   "source": [
    "#### W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f5a8637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1.]]), tensor([[1., 1., 0.]], requires_grad=True))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[0,1]], dtype=torch.float32)\n",
    "y = torch.tensor([[1,1,0]], dtype=torch.float32, requires_grad=True)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d44eaf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1., -1.]]),\n",
       " tensor([[1.0000, 1.0000, 0.8000]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.tensor([0.8], dtype=torch.float32)\n",
    "\n",
    "first_factor = 1 - 2*x\n",
    "second_factor = Q + (1-Q) * y\n",
    "first_factor, second_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c000261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_factor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "992b6c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  1.0000,  0.8000],\n",
       "         [-1.0000, -1.0000, -0.8000]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.reshape(first_factor, first_factor.shape + (1,)),  \n",
    "         torch.reshape(second_factor, (second_factor.shape[0], 1) + second_factor.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db29d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in gx\n",
    "gx = torch.tensor([[1,0]], dtype=torch.float32)\n",
    "first_factor = gx * (1 - 2*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33bc67e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 1.0000, 0.8000],\n",
       "         [0.0000, 0.0000, 0.0000]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.reshape(first_factor, first_factor.shape + (1,)),  \n",
    "         torch.reshape(second_factor, (second_factor.shape[0], 1) + second_factor.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f187991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W(gx, x, y, q):\n",
    "    first_factor = (-1) * gx * (1 - 2*x)\n",
    "    second_factor = q + (1-q) * y\n",
    "    return torch.matmul(torch.reshape(first_factor, first_factor.shape + (1,)),  \n",
    "                        torch.reshape(second_factor, (second_factor.shape[0], 1) + second_factor.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44b86f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -1.0000, -0.8000],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W(gx,x,y,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a60c837",
   "metadata": {},
   "source": [
    "#### G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ec62911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.,  1.]]), tensor([[ 1.,  1., -1.]], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[0,1]], dtype=torch.float32)\n",
    "y = torch.tensor([[1,1,0]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "seq_first_factor = 2*x - 1\n",
    "seq_second_factor = 2*y - 1\n",
    "seq_first_factor, seq_second_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14f16ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq(x, y):\n",
    "    seq_first_factor = 2*x - 1\n",
    "    seq_second_factor = 2*y - 1\n",
    "    return torch.matmul(torch.reshape(seq_first_factor, seq_first_factor.shape +(1,)),  \n",
    "             torch.reshape(seq_second_factor, (seq_second_factor.shape[0], 1) + seq_second_factor.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb883a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1., -1.,  1.],\n",
       "         [ 1.,  1., -1.]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8dfa3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU = torch.nn.ReLU()\n",
    "\n",
    "def G(gx, x, y, w):\n",
    "    new_w = seq(x, y) * torch.reshape(w, (1,) + w.shape) \n",
    "    linear = torch.matmul(torch.reshape(gx, (gx.shape[0], 1) + gx.shape[1:]), new_w)\n",
    "    return ReLU(torch.reshape(linear, (linear.shape[0],) + linear.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a383cc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gx : tensor([[1., 0.]]) \n",
      " x: tensor([[0., 1.]]) \n",
      " y: tensor([[1., 1., 0.]], requires_grad=True)\n",
      " w: tensor([[ 0.2000,  0.1000,  1.1000],\n",
      "        [ 0.0000, -0.4000,  0.3000]], requires_grad=True) \n",
      " B: tensor([0.5000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 1.1000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([0.5])\n",
    "print(\"gx : {} \\n x: {} \\n y: {}\\n w: {} \\n B: {}\".format(gx, x, y, weights, B))\n",
    "G(gx, x, y, weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c23fe8",
   "metadata": {},
   "source": [
    "#### Oja's rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ebd0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: y are the inputs, x the outputs\n",
    "\n",
    "def oja(x, y, w, B, epsilon=torch.tensor(0.001)):\n",
    "    x_reshape = torch.reshape(x, x.shape+(1,))\n",
    "    first_matrix = x_reshape * x_reshape * w\n",
    "    second_matrix =  (1 + epsilon) * B* torch.matmul(x_reshape, torch.reshape(y, (y.shape[0], 1) + y.shape[1:]))\n",
    "    return first_matrix-second_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63d622f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000],\n",
       "         [0.4500, 0.1315]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_oja = torch.tensor([[1.0276, 0.3],\n",
    "        [0.45, 1.1325]], requires_grad=True)\n",
    "x_oja = torch.tensor([[0., 1.]])\n",
    "y_oja = torch.tensor([[0., 1.]])\n",
    "B = torch.tensor([1.])\n",
    "\n",
    "oja(x_oja, y_oja, w_oja, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a0c69",
   "metadata": {},
   "source": [
    "#### Generalized Hebbian algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ebd0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: y are the inputs, x the outputs\n",
    "\n",
    "def hebbian(x, y, w, B, epsilon=torch.tensor(0.5)):\n",
    "    x_reshape = torch.reshape(x, x.shape+(1,))\n",
    "    vector = torch.matmul(x, w)\n",
    "    first_matrix = torch.matmul(x_reshape, torch.reshape(vector, (vector.shape[0], 1) + vector.shape[1:]))   \n",
    "    second_matrix =  (1 + epsilon) * B* torch.matmul(x_reshape, torch.reshape(y, (y.shape[0], 1) + y.shape[1:]))\n",
    "    return first_matrix-second_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e811769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000],\n",
       "         [ 0.4500, -0.3675]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_heb = torch.tensor([[1.0276, 0.3],\n",
    "        [0.45, 1.1325]], requires_grad=True)\n",
    "x_heb = torch.tensor([[0., 1.]])\n",
    "y_heb = torch.tensor([[0., 1.]])\n",
    "B = torch.tensor([1.])\n",
    "\n",
    "hebbian(x_heb, y_heb, w_heb, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a5be5",
   "metadata": {},
   "source": [
    "#### Autograd function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e877099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBool(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    # B and Q are added as tensors\n",
    "    def forward(ctx, input, weight, b, q, oja_lr, heb_lr):\n",
    "        output = forward_bool(weight, input, b)\n",
    "        #print(\"In forward: {}\".format(output))\n",
    "        ctx.save_for_backward(input, weight, b, q, oja_lr, heb_lr, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, b, q, oja_lr, heb_lr, output = ctx.saved_tensors\n",
    "\n",
    "        grad_input = grad_weight = grad_b = grad_q = grad_oja_lr = grad_heb_lr = None\n",
    "\n",
    "        grad_input = G(grad_output, output, input, weight)\n",
    "        grad_weight = W(grad_output, output, input, q)\n",
    "\n",
    "        #additional \"learning\", independent of gradients\n",
    "        if oja_lr!=0.:\n",
    "            grad_weight += oja_lr *  oja(output, input, weight, b)  \n",
    "            #print(oja(output, input, weight, b))\n",
    "\n",
    "        if heb_lr!=0.:\n",
    "            grad_weight += heb_lr *  hebbian(output, input, weight, b)  \n",
    "    \n",
    "        return grad_input, grad_weight, grad_b, grad_q, grad_oja_lr, grad_heb_lr\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1934a20",
   "metadata": {},
   "source": [
    "##### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c268ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]], grad_fn=<LinearBoolBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward\n",
    "\n",
    "linear = LinearBool.apply\n",
    "#print(y, weights, B, Q)\n",
    "oja_lr = torch.tensor([0.])\n",
    "\n",
    "output = linear(y, weights, B, Q, torch.tensor([0.1]), torch.tensor([0.]))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb89903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward\n",
    "desired_output = torch.tensor([[1, 1]], dtype=torch.float32)\n",
    "gradient = neq(output, desired_output)\n",
    "\n",
    "output.backward(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bd494e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0000, -1.0000, -0.8000],\n",
       "         [-1.0000, -1.0000, -0.8000]]),\n",
       " tensor([[0.0000, 0.3000, 1.4000]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c1282",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "In order to compare performance with a non binary approach, we introduce a simple linear layer with constant bias and ReLU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ceae815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_compare(y, weights, B):\n",
    "    l = torch.matmul(y, weights.t()) - B\n",
    "    return torch.nn.ReLU()(l)\n",
    "\n",
    "def loss_gradient(o, output_bn):\n",
    "    return output_bn-o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c1282",
   "metadata": {},
   "source": [
    "#### Models helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bebc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, Q=0.05, B=1., oja_lr=0., heb_lr = 0., weights=None):\n",
    "        super(LinearBN, self).__init__()\n",
    "\n",
    "        self.Q = torch.tensor([Q], dtype=torch.float32)\n",
    "        self.B = torch.tensor([B], dtype=torch.float32)\n",
    "        self.oja_lr = torch.tensor([oja_lr], dtype=torch.float32)\n",
    "        self.heb_lr = torch.tensor([heb_lr], dtype=torch.float32)\n",
    "\n",
    "        # TODO correct scale and initialization?\n",
    "        scale = 2*B\n",
    "        \n",
    "        if weights is None:\n",
    "            self.weights = torch.nn.Parameter(weights_init((out_features, in_features), scale, self.B))\n",
    "        else: \n",
    "            self.weights = torch.nn.Parameter(weights)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # transform to batch\n",
    "        if len(input.shape) == 1:\n",
    "            input = torch.reshape(input, (1,) + input.shape)\n",
    "        return LinearBool.apply(input, self.weights, self.B, self.Q, self.oja_lr, self.heb_lr)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f05bbcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = LinearBN(2, 3, Q=0.01)\n",
    "s = LinearB(2, 3, Q=torch.tensor([0.01]), B=1.)\n",
    "\n",
    "with torch.no_grad():\n",
    "    s.core.weight = torch.nn.Parameter(t.weights.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9d808b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1.6618, 1.6966],\n",
      "        [2.1588, 0.7884],\n",
      "        [1.8769, 1.8643]], requires_grad=True) Parameter containing:\n",
      "tensor([[1.6618, 1.6966],\n",
      "        [2.1588, 0.7884],\n",
      "        [1.8769, 1.8643]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(t.weights, s.core.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5b61a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "batch = weights_init((batch_size, 2), 1., -0.5 * B)\n",
    "with torch.no_grad():\n",
    "    batch = H(batch)\n",
    "batch.requires_grad = True\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a7c75c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 1.]], grad_fn=<LinearBoolBackward>)\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 1.]], grad_fn=<HeavisideBNBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because of the leakyness, the forward passes will not be always the same\n",
    "\n",
    "out_t = t(batch)\n",
    "print(out_t) \n",
    "out_s = s(batch)\n",
    "print(out_s)\n",
    "\n",
    "out_t == out_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4187c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_t = torch.optim.SGD(t.parameters(), lr=0.1)\n",
    "optimizer_s = torch.optim.SGD(s.parameters(), lr=0.1)\n",
    "optimizer_t.zero_grad()\n",
    "optimizer_s.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6017132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired = torch.tensor([[1., 0., 1.],\n",
    "        [1., 0., 0.],\n",
    "        [1., 0., 0.],\n",
    "        [1., 1., 0.],\n",
    "        [1., 1., 1.],\n",
    "        [0., 1., 1.],\n",
    "        [0., 0., 0.],\n",
    "        [1., 0., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16dfb7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [0., 1., 1.],\n",
       "        [0., 1., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 1.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #assert torch.all(out_t==out_s), \"Same outputs needed\"\n",
    "    grad = neq(out_t, desired)\n",
    "\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c485e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t.backward(grad)\n",
    "out_s.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4892e654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.weights.grad == s.core.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f6bebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_t.step()\n",
    "optimizer_s.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.weights==s.core.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0c007",
   "metadata": {},
   "source": [
    "#### Learning tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96003491",
   "metadata": {},
   "source": [
    "##### Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c4543e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def learn(inputs, desired_outputs, model, weights, bias, learning_rate, \n",
    "            binarizatio_fn=lambda input: torch.heaviside(input, torch.tensor([0.])),\n",
    "            loss_gradient = neq, print_it=False):\n",
    "\n",
    "    for w in weights:\n",
    "        w.grad = None\n",
    "\n",
    "    for b in bias:\n",
    "        b.grad = None\n",
    "\n",
    "    eq_counter = 0\n",
    "    for i, o in zip(inputs, desired_outputs):\n",
    "        output = model(i)\n",
    "        output_bn = binarizatio_fn(output)\n",
    "        if print_it:\n",
    "            with torch.no_grad():\n",
    "                print(\"{} -> {}\".format(i.numpy(), output_bn.numpy()))\n",
    "\n",
    "        if torch.all(o==output_bn):\n",
    "            eq_counter += 1\n",
    "            \n",
    "        gradient = loss_gradient(o, output_bn)\n",
    "            \n",
    "        output.backward(gradient)\n",
    "        with torch.no_grad():\n",
    "            for w in weights:\n",
    "                if print_it:\n",
    "                    print(\"Weight grads: {}\".format(w.grad))\n",
    "                w -= learning_rate * w.grad\n",
    "                w.grad = None\n",
    "            for b in bias:\n",
    "                b -= learning_rate * b.grad\n",
    "                b.grad = None\n",
    "                    \n",
    "    if eq_counter == len(inputs):\n",
    "        return \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c4543e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_model(inputs, desired_outputs, model, learning_rate, \n",
    "            binarizatio_fn=lambda input: torch.heaviside(input, torch.tensor([0.])),\n",
    "            loss_gradient = neq, print_it=False):\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    eq_counter = 0\n",
    "    for i, o in zip(inputs, desired_outputs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(i)\n",
    "        output_bn = binarizatio_fn(output)\n",
    "        \n",
    "        if print_it:\n",
    "            with torch.no_grad():\n",
    "                print(\"{} -> {}\".format(i.numpy(), output_bn.numpy()))\n",
    "\n",
    "        if torch.all(o==output_bn):\n",
    "            eq_counter += 1\n",
    "        \n",
    "        gradient = loss_gradient(o, output_bn)\n",
    "            \n",
    "        output.backward(gradient)\n",
    "        if print_it:\n",
    "            for w in model.parameters():\n",
    "                print(\"Weight grads: {}\".format(w.grad))\n",
    "        optimizer.step()\n",
    "                    \n",
    "    if eq_counter == len(inputs):\n",
    "        return \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438fe0c",
   "metadata": {},
   "source": [
    "##### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4543e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_unsupervised(inputs, model, learning_rate, print_it=False):\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for i in inputs:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(i)\n",
    "        \n",
    "        if print_it:\n",
    "            with torch.no_grad():\n",
    "                print(\"{} -> {}\".format(i.numpy(), output.numpy()))\n",
    "        \n",
    "        gradient = torch.zeros(output.shape, dtype=output.dtype)\n",
    "            \n",
    "        output.backward(gradient)\n",
    "        if print_it:\n",
    "            for w in model.parameters():\n",
    "                print(\"Weight grads: {}\".format(w.grad))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe65a9c",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "All examples start with random initialisation of weights in the range $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15325304",
   "metadata": {},
   "source": [
    "#### Copy one neuron\n",
    "\n",
    "Architecture: `input_neuron -> output_neuron` \n",
    "\n",
    "Desired behavior: `0->0, 1->1`\n",
    "\n",
    "Explicitly, for forward $y=H(x\\cdot w-B)$. If $x=0$ then $y=0$, which is desired behavior. Suppose $x=1$ and $y=0$. Desired value is $y'=1$. For backward: the first gradient is $gy= Eq(y', y)=1$. The weight gradient reads $gw = W(gy, y, x)= 1$, pointing to increasing $w$, which is correct behavior. \n",
    "\n",
    "Once correct behavior is established, the weight gradients are zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "202e1e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with weight tensor([-0.6000], requires_grad=True)\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradient: tensor([-1.])\n",
      "Changing weight to tensor([0.4000], requires_grad=True)\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradient: tensor([-1.])\n",
      "Changing weight to tensor([1.4000], requires_grad=True)\n",
      "Output: tensor([1.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradient: tensor([0.0080])\n",
      "Changing weight to tensor([1.3920], requires_grad=True)\n",
      "Output: tensor([1.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradient: tensor([0.0078])\n",
      "Changing weight to tensor([1.3842], requires_grad=True)\n",
      "Output: tensor([1.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradient: tensor([0.0077])\n",
      "Changing weight to tensor([1.3765], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1.], requires_grad=True)\n",
    "desired_output = torch.tensor([1.], requires_grad=True)\n",
    "# start with negative weight, it will become positive after learning \n",
    "w = torch.tensor([-0.6], requires_grad=True)\n",
    "print(\"Starting with weight {}\".format(w))\n",
    "gradient = None\n",
    "limit = 5\n",
    "counter = 0\n",
    "\n",
    "while  counter<limit:\n",
    "    output = linear(y, w, B, Q, torch.tensor([0.02]), torch.tensor([0.]))\n",
    "    print(\"Output: {}\".format(output))\n",
    "\n",
    "    gradient = neq(output, desired_output)\n",
    "    output.backward(gradient)\n",
    "    print(\"Weight gradient: {}\".format(w.grad))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad\n",
    "    print(\"Changing weight to {}\".format(w))\n",
    "    w.grad = None\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae87f7c",
   "metadata": {},
   "source": [
    "#### Copy one neuron with a neuron in between\n",
    "\n",
    "Architecture: `input_neuron -> other_neuron -> output_neuron` \n",
    "\n",
    "Desired behavior: `0->0, 1->1`\n",
    "\n",
    "The backpropagation of \n",
    "$$\n",
    "x_\\text{in} \\xrightarrow{w_\\text{in}} x_\\text{other} \\xrightarrow{w_\\text{other}} x_{\\text{out}}\n",
    "$$\n",
    "is computed as follows\n",
    "$$\n",
    "gx_\\text{out} \\xrightarrow{gw_\\text{other}} gx_\\text{other} \\xrightarrow{gw_\\text{in}} gx_\\text{in}$$\n",
    "Independent of the $x_{\\text{other}}$ values, $gw_\\text{other}$ will be positive if the desired value for $x_\\text{out}$ is not reached. Once $w_\\text{other}$ is large enough, $gx_\\text{other}$ will become positive if $x_\\text{out}=0$ is not desired and $x_{\\text{other}}=0$. This will push $gw_\\text{in}$ to positive, fixing $w_\\text{in}$. The model will solve the task.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30c7f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with weights [tensor([-0.6000], requires_grad=True), tensor([-0.9000], requires_grad=True)]\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradients: [tensor([0.]), tensor([-0.8000])]\n",
      "Changing weight to [tensor([-0.6000], requires_grad=True), tensor([-0.1000], requires_grad=True)]\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradients: [tensor([0.]), tensor([-0.8000])]\n",
      "Changing weight to [tensor([-0.6000], requires_grad=True), tensor([0.7000], requires_grad=True)]\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradients: [tensor([-0.7000]), tensor([-0.8000])]\n",
      "Changing weight to [tensor([0.1000], requires_grad=True), tensor([1.5000], requires_grad=True)]\n",
      "Output: tensor([0.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradients: [tensor([-1.5000]), tensor([-0.8000])]\n",
      "Changing weight to [tensor([1.6000], requires_grad=True), tensor([2.3000], requires_grad=True)]\n",
      "Output: tensor([1.], grad_fn=<LinearBoolBackward>)\n",
      "Weight gradients: [tensor([0.]), tensor([0.])]\n",
      "Changing weight to [tensor([1.6000], requires_grad=True), tensor([2.3000], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1.], requires_grad=True)\n",
    "desired_output = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "# start with negative weight, it will become positive after learning \n",
    "w_in = torch.tensor([-0.6], requires_grad=True)\n",
    "w_other = torch.tensor([-0.9], requires_grad=True)\n",
    "weights = [w_in, w_other]\n",
    "print(\"Starting with weights {}\".format(weights))\n",
    "\n",
    "def model(input): \n",
    "    return linear(linear(input, w_in, B, Q, torch.tensor([0.]), torch.tensor([0.])), \n",
    "                                w_other, B, Q, torch.tensor([0.]), torch.tensor([0.]))\n",
    "\n",
    "gradient = None\n",
    "limit = 10\n",
    "counter = 0\n",
    "\n",
    "while gradient != torch.tensor([0.]) and counter < limit:\n",
    "    output = model(y)\n",
    "    print(\"Output: {}\".format(output))\n",
    "\n",
    "    gradient = neq(output, desired_output)\n",
    "    output.backward(gradient)\n",
    "    print(\"Weight gradients: {}\".format([w.grad for w in weights]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for w in weights:\n",
    "            w -= w.grad\n",
    "            w.grad = None            \n",
    "    print(\"Changing weight to {}\".format(weights))\n",
    " \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d241950",
   "metadata": {},
   "source": [
    "#### Always disagree with one neuron\n",
    "\n",
    "Architecture: `input_neuron -> output_neuron` \n",
    "\n",
    "Desired behavior: `1->0`. Note that `0->0` is automatic. \n",
    "\n",
    "Explicitly, for forward $y=H(x\\cdot w-B)$. If $x=1$ and $y=1$, then the weight gradient is\n",
    "$\n",
    "gw = -1\n",
    "$\n",
    "pushing the weights in the negative direction, which is correct behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a598200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New weights: [tensor([0.9000], requires_grad=True)]\n",
      "[1.] -> [0.]\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([1.], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([0.])]\n",
    "weights = [torch.tensor([1.], requires_grad=True)]\n",
    "\n",
    "def model(input):\n",
    "    return linear(input, weights[0], B, Q, torch.tensor([0.]), torch.tensor([0.]))\n",
    "\n",
    "for i in range(10):\n",
    "    learn(inputs, desired_outputs, model, weights, [], learning_rate=0.1)\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f1f37",
   "metadata": {},
   "source": [
    "#### Switch positions\n",
    "\n",
    "Architecture: `input_neuron_0, input_neuron_1  -> output_neuron_0, output_neuron_1` \n",
    "\n",
    "Desired behavior: `I:(1,0)->(0,1), II:(0,1)->(1,0), III:(1,1)->(1,1)`\n",
    "\n",
    "Explicitly, for forward $y_i=H(x_0\\cdot w_{i,0} + x_1\\cdot w_{i,1} - B)$. Let's consider the weights one by one:\n",
    "* $w_{0,0}$: since `I` and `III` send contradictory impulses at first, no changes.\n",
    "* $w_{0,1}$: increasing, so it will fix `II` and have a strong angle on `III`. This will allow $w_{0,0}$ to focus on `I` and fix it. \n",
    "* $w_{1,0}$: Same as $w_{0,1}$.\n",
    "* $w_{1,1}$: Same as $w_{0,0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3014401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done after epoch 15\n",
      "New weights: [tensor([[0.9186, 1.0154],\n",
      "        [1.0007, 0.9854]], requires_grad=True)]\n",
      "[[1. 0.]] -> [[0. 1.]]\n",
      "[[0. 1.]] -> [[1. 0.]]\n",
      "[[1. 1.]] -> [[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True),\n",
    "          torch.tensor([[1., 1.]], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([[0., 1.]]), torch.tensor([[1., 0.]]), torch.tensor([[1., 1.]])]\n",
    "\n",
    "# initialization must be random\n",
    "weights = [torch.tensor([[0.1, -0.2], \n",
    "                         [-0.22, -0.01]], requires_grad=True)]\n",
    "\n",
    "def model(input):\n",
    "    return linear(input, weights[0], B, Q, torch.tensor([0.01]), torch.tensor([0.]))\n",
    "\n",
    "for i in range(50):\n",
    "    done = learn(inputs, desired_outputs, model, weights, [],  learning_rate=0.1)\n",
    "    if done==\"Done\":\n",
    "        print(\"Done after epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b126fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done after epoch 12\n",
      "New weights: [tensor([[0.1000, 0.2000],\n",
      "        [0.2000, 0.1000]], requires_grad=True)]\n",
      "New bias: []\n",
      "[[1. 0.]] -> [[0. 1.]]\n",
      "[[0. 1.]] -> [[1. 0.]]\n",
      "[[1. 1.]] -> [[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Comparison with ReLUs \n",
    "\n",
    "# initialization must be random\n",
    "# Following weights won't work, because ReLU does not propagate grads if value = 0\n",
    "# weights = [torch.tensor([[0.1, -0.2], \n",
    "#                         [-0.22, -0.01]], requires_grad=True)]\n",
    "weights = [torch.tensor([[0.1, 0.13], \n",
    "                            [0.05, 0.07]], requires_grad=True)]\n",
    "\n",
    "#bias = [torch.tensor([0.5], requires_grad=True)]\n",
    "bias = []\n",
    "binarization_fn = lambda input: torch.heaviside(input-0.2, torch.tensor([0.]))\n",
    "\n",
    "def model(input):\n",
    "    return linear_compare(input, weights[0], torch.tensor([0.])) \n",
    "\n",
    "for i in range(20):\n",
    "    done = learn(inputs, desired_outputs, model, weights, bias, learning_rate=0.01, \n",
    "                    binarizatio_fn=binarization_fn, loss_gradient=loss_gradient)\n",
    "    #done = learn(inputs, desired_outputs, model, weights, bias, learning_rate=0.1)\n",
    "    if done==\"Done\":\n",
    "        print(\"Done after epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "print(\"New bias: {}\".format(bias))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), binarization_fn(model(input)).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c3014401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New weights: Parameter containing:\n",
      "tensor([[0.5045, 1.0056],\n",
      "        [1.0091, 0.5053]], requires_grad=True)\n",
      "[[1. 0.]] -> [[0. 1.]]\n",
      "[[0. 1.]] -> [[1. 0.]]\n",
      "[[1. 1.]] -> [[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# with model class definition\n",
    "model = LinearBN(2,2, oja_lr=0.04)\n",
    "\n",
    "for i in range(600):\n",
    "    done = learn_model(inputs, desired_outputs, model, learning_rate=0.1)\n",
    "    #if done==\"Done\":\n",
    "    #    print(\"Done after epoch {}\".format(i))\n",
    "    #    break\n",
    "\n",
    "print(\"New weights: {}\".format(model.weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94f3f7",
   "metadata": {},
   "source": [
    "#### Hidden representation\n",
    "\n",
    "Architecture: `input_0, input_1  -> hidden_0, hidden_1, hidden_2 -> output` \n",
    "\n",
    "Desired behavior: `I:(1,0)->(1), II:(0,1)->(1), III:(1,1)->(0)`\n",
    "\n",
    "This cannot be learned with one layer only, because `I` and `II` imply two weights that are larger than the bias. This contradicts `III`.\n",
    "\n",
    "Questions\n",
    "* Why is $Q=0.1$ working, but other values are not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed424cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in epoch 46\n",
      "New weights: [tensor([[1.0068, 0.9689],\n",
      "        [1.0170, 1.0247],\n",
      "        [0.6346, 0.4538]], requires_grad=True), tensor([[ 0.1440,  1.0950, -0.2550]], requires_grad=True)]\n",
      "[[1. 0.]] -> [[1.]]\n",
      "[[0. 1.]] -> [[1.]]\n",
      "[[1. 1.]] -> [[0.]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True),\n",
    "          torch.tensor([[1., 1.]], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([[1.]]), torch.tensor([[1.]]), torch.tensor([[0.]])]\n",
    "\n",
    "use_Q = torch.tensor([0.1])\n",
    "\n",
    "# initialization must be random\n",
    "weights = [torch.tensor([[0.1, -0.2], \n",
    "                         [-0.22, -0.01],\n",
    "                         [0.22, 0.01]], requires_grad=True), \n",
    "           torch.tensor([[0.1, 0.16, -0.2]], requires_grad=True)]\n",
    "\n",
    "def model(input):\n",
    "    hidden = linear(input, weights[0], B, use_Q, torch.tensor([0.]), torch.tensor([0.]))\n",
    "    output = linear(hidden, weights[1], B, use_Q, torch.tensor([0.]), torch.tensor([0.]))\n",
    "    return output\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for i in range(epochs):\n",
    "    done = learn(inputs, desired_outputs, model, weights, [], learning_rate=0.11)\n",
    "    if done == \"Done\":\n",
    "        print(\"Finished in epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c7025",
   "metadata": {},
   "source": [
    "#### Hidden representation 2\n",
    "\n",
    "Architecture: `input_0, input_1  -> hidden_0, hidden_1 -> output` \n",
    "\n",
    "Desired behavior: `I:(1,0)->(1), II:(0,1)->(1), III:(1,1)->(0)`\n",
    "\n",
    "This cannot be learned with one layer only, because `I` and `II` imply two weights that are larger than the bias. This contradicts `III`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91707287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in epoch 128\n",
      "New weights: [tensor([[0.2018, 0.2015],\n",
      "        [0.0383, 0.2000]], requires_grad=True), tensor([[ 0.2002, -0.0044]], requires_grad=True)]\n",
      "[[1. 0.]] -> [[1.]]\n",
      "[[0. 1.]] -> [[1.]]\n",
      "[[1. 1.]] -> [[0.]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True),\n",
    "          torch.tensor([[1., 1.]], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([[1.]]), torch.tensor([1.]), torch.tensor([[0.]])]\n",
    "\n",
    "use_Q = torch.tensor([0.1])\n",
    "B=torch.tensor([0.2])\n",
    "\n",
    "# initialization must be random\n",
    "weights = [weights_init((2, 2), 2*B, B), \n",
    "           weights_init((1, 2), 2*B, B)]\n",
    "\n",
    "def model(input, B=B):\n",
    "    hidden = linear(input, weights[0], B, use_Q, torch.tensor([0.]), torch.tensor([0.]))\n",
    "    output = linear(hidden, weights[1], B, use_Q, torch.tensor([0.]), torch.tensor([0.]))\n",
    "    return output\n",
    "\n",
    "epochs = 3000\n",
    "\n",
    "for i in range(epochs):\n",
    "    done = learn(inputs, desired_outputs, model, weights, [], learning_rate=0.01)\n",
    "    if done == \"Done\":\n",
    "        print(\"Finished in epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cbf1870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparison with ReLUs\n",
    "\n",
    "def hidden_representation_2_relu(epochs = 10000):\n",
    "    inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True),\n",
    "          torch.tensor([[1., 1.]], requires_grad=True)]\n",
    "    desired_outputs = [torch.tensor([[1.]]), torch.tensor([[1.]]), torch.tensor([[0.]])]\n",
    "\n",
    "    bias = []\n",
    "    binarization_fn = lambda input: torch.heaviside(input-0.2, torch.tensor([0.]))\n",
    "\n",
    "# initialization must be random\n",
    "    weights = [torch.tensor([[0.1, 0.2], \n",
    "                         [0.22, 0.1]], requires_grad=True), \n",
    "           torch.tensor([[0.1, 0.16]], requires_grad=True)]\n",
    "\n",
    "    zero = torch.tensor([0.])\n",
    "\n",
    "    def model(input):\n",
    "        hidden = linear_compare(input, weights[0], zero)\n",
    "        output = linear_compare(hidden, weights[1], zero)\n",
    "        return output\n",
    "\n",
    "    for i in range(epochs):\n",
    "        done = learn(inputs, desired_outputs, model, weights, bias, learning_rate=0.05,\n",
    "                    binarizatio_fn=binarization_fn, loss_gradient=loss_gradient)\n",
    "        if done == \"Done\":\n",
    "            print(\"Finished in epoch {}\".format(i))\n",
    "            break\n",
    "\n",
    "    print(\"New weights: {}\".format(weights))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in inputs:\n",
    "            print(\"{} -> {}\".format(input.numpy(), binarization_fn(model(input)).numpy()))\n",
    "\n",
    "#hidden_representation_2_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "36b08c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison with model class approach\n",
    "\n",
    "def hidden_representation_2_bn_model_class(epochs = 10000):\n",
    "    inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True),\n",
    "          torch.tensor([[1., 1.]], requires_grad=True)]\n",
    "    desired_outputs = [torch.tensor([[1.]]), torch.tensor([1.]), torch.tensor([[0.]])]\n",
    "\n",
    "    use_Q = torch.tensor([0.01])\n",
    "\n",
    "    model = torch.nn.Sequential(LinearBN(2, 2, Q=use_Q, B=1.),\n",
    "                                LinearBN(2, 1, Q=use_Q, B=1.))\n",
    "\n",
    "    for i in range(epochs):\n",
    "        done = learn_model(inputs, desired_outputs, model, learning_rate=0.01)\n",
    "        if done == \"Done\":\n",
    "            print(\"Finished in epoch {}\".format(i))\n",
    "            break\n",
    "\n",
    "    print(\"New weights: {}\".format(list(model.parameters())))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in inputs:\n",
    "            print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))\n",
    "\n",
    "#hidden_representation_2_bn_model_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36b08c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison with wrapped linear approach\n",
    "\n",
    "#from turtle import forward\n",
    "\n",
    "\n",
    "def hidden_representation_2_wrapped_bn(epochs = 10):\n",
    "    inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True),\n",
    "          torch.tensor([[1., 1.]], requires_grad=True)]\n",
    "    desired_outputs = [torch.tensor([[1.]]), torch.tensor([1.]), torch.tensor([[0.]])]\n",
    "\n",
    "    use_Q = torch.tensor([0.01])\n",
    "\n",
    "    class HR2Wrapped(torch.nn.Module):\n",
    "\n",
    "        def __init__(self, Q=use_Q, B=1.):\n",
    "            super(HR2Wrapped, self).__init__()\n",
    "            self.first = LinearB(2, 2, Q=use_Q, B=1.)\n",
    "            self.second = LinearB(2, 1, Q=use_Q, B=1.)\n",
    "        \n",
    "        def forward(self, inputs):\n",
    "            x = self.first(inputs)\n",
    "            x = self.second(x)\n",
    "            return x\n",
    "\n",
    "    model = HR2Wrapped()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        done = learn_model(inputs, desired_outputs, model, learning_rate=0.01)\n",
    "        if done == \"Done\":\n",
    "            print(\"Finished in epoch {}\".format(i))\n",
    "            break\n",
    "\n",
    "    print(\"New weights: {}\".format(list(model.parameters())))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in inputs:\n",
    "            print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))\n",
    "\n",
    "#hidden_representation_2_wrapped_bn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c7025",
   "metadata": {},
   "source": [
    "#### Locked state\n",
    "\n",
    "Architecture: `input_0, input_1  -> hidden_0, hidden_1 -> output` \n",
    "\n",
    "Desired behavior: `I:(1,0)->(1), II:(0,1)->(0)`\n",
    "\n",
    "Initial weights: such that `(1,0)` and `(0,1)` are mapped to the same element in the hidden layer.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db0aa48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [tensor([[ 0.0300,  1.9800],\n",
      "        [-0.0100,  0.7500]], requires_grad=True), tensor([[0.2070, 0.3200]], requires_grad=True)]\n",
      "Finished in epoch 56\n",
      "New weights: [tensor([[0.9946, 2.0765],\n",
      "        [1.0122, 0.8522]], requires_grad=True), tensor([[0.4870, 1.0050]], requires_grad=True)]\n",
      "[[1. 0.]] -> [[1.]]\n",
      "[[0. 1.]] -> [[0.]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([[1.]]), torch.tensor([[0.]])]\n",
    "\n",
    "use_Q = torch.tensor([0.1])\n",
    "\n",
    "# initialization as required by the problem\n",
    "weights = [torch.tensor([[0.03, 1.98],\n",
    "                            [-0.01, 0.75]], requires_grad=True), \n",
    "           torch.tensor([[0.2070, 0.32]], requires_grad=True)]\n",
    "\n",
    "print(\"Initial weights: {}\".format(weights))\n",
    "\n",
    "def model(input, B=torch.tensor([1.])):\n",
    "    hidden = linear(input, weights[0], B, use_Q, torch.tensor([0.]), torch.tensor([0.]))\n",
    "    output = linear(hidden, weights[1], B, use_Q, torch.tensor([0.]), torch.tensor([0.]))\n",
    "    return output\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    #print(\"Epoch: {}\".format(i))\n",
    "    #print(\"Start weights: {}\".format(weights))\n",
    "    done = learn(inputs, desired_outputs, model, weights, [], learning_rate=0.05)\n",
    "    if done == \"Done\":\n",
    "        print(\"Finished in epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cbf1870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in epoch 15\n",
      "New weights: [tensor([[ 0.4596,  0.2300],\n",
      "        [-0.0100,  0.0400]], requires_grad=True), tensor([[0.4489, 0.3200]], requires_grad=True)]\n",
      "[[1. 0.]] -> [[1.]]\n",
      "[[0. 1.]] -> [[0.]]\n"
     ]
    }
   ],
   "source": [
    "### Comparison with ReLUs\n",
    "\n",
    "inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True)]\n",
    "desired_outputs = [torch.tensor([[1.]]), torch.tensor([[0.]])]\n",
    "\n",
    "bias = []\n",
    "binarization_fn = lambda input: torch.heaviside(input-0.2, torch.tensor([0.]))\n",
    "\n",
    "# initialization as required by the problem\n",
    "weights = [torch.tensor([[0.23, 0.23],\n",
    "                            [-0.01, 0.04]], requires_grad=True), \n",
    "           torch.tensor([[0.2070, 0.32]], requires_grad=True)]\n",
    "\n",
    "zero = torch.tensor([0.])\n",
    "\n",
    "def model(input):\n",
    "    hidden = linear_compare(input, weights[0], zero)\n",
    "    output = linear_compare(hidden, weights[1], zero)\n",
    "    return output\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for i in range(epochs):\n",
    "    done = learn(inputs, desired_outputs, model, weights, bias, learning_rate=0.05,\n",
    "                    binarizatio_fn=binarization_fn, loss_gradient=loss_gradient)\n",
    "    if done == \"Done\":\n",
    "        print(\"Finished in epoch {}\".format(i))\n",
    "        break\n",
    "\n",
    "print(\"New weights: {}\".format(weights))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), binarization_fn(model(input)).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c7025",
   "metadata": {},
   "source": [
    "#### Getting through layers\n",
    "\n",
    "Architecture: `input_0, input_1  -> layer_1 -> layer_2 -> ... -> layer_d` \n",
    "\n",
    "All layers are `2`-dimensional, and the depth `d` is variable. \n",
    "\n",
    "Desired behavior: `(1,0)->(1,0), (0,1)->(0,1)`\n",
    "\n",
    "Initial weights: random\n",
    "\n",
    "The task is to get through all the randomly initialized layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db0aa48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_through_layers_old_bn(d = 5, epochs = 10000):\n",
    "    inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True)]\n",
    "    desired_outputs = [torch.tensor([[1., 0.]]), torch.tensor([[0., 1.]])]\n",
    "\n",
    "# constants\n",
    "    use_Q = torch.tensor([0.05])\n",
    "    b = 0.2 \n",
    "\n",
    "# random initialization\n",
    "    weights = []\n",
    "    for i in range(d):\n",
    "        weights.append(weights_init((2, 2), 0.2, b))\n",
    "\n",
    "    print(\"Initial weights: {}\".format(weights))\n",
    "\n",
    "    def model(input, B=torch.tensor([b])):\n",
    "        all_layers = {0: input}\n",
    "        for i in range(d):\n",
    "            all_layers[i+1] = linear(all_layers[i], weights[i], B, use_Q, torch.tensor([0.]), torch.tensor([0.]))\n",
    "        return all_layers[d]\n",
    "\n",
    "    for i in range(epochs):\n",
    "    #print(\"Epoch: {}\".format(i))\n",
    "    #print(\"Start weights: {}\".format(weights))\n",
    "        done = learn(inputs, desired_outputs, model, weights, [], learning_rate=0.1)\n",
    "        if done == \"Done\":\n",
    "            print(\"Finished in epoch {}\".format(i))\n",
    "            break\n",
    "\n",
    "    print(\"New weights: {}\".format(weights))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in inputs:\n",
    "            print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))\n",
    "\n",
    "#getting_through_layers_old_bn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db0aa48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With ReLUs\n",
    "\n",
    "def getting_through_layers_relu(d = 5, epochs = 10000):\n",
    "    inputs = [torch.tensor([[1., 0.]], requires_grad=True), \n",
    "          torch.tensor([[0., 1.]], requires_grad=True)]\n",
    "    desired_outputs = [torch.tensor([[1., 0.]]), torch.tensor([[0., 1.]])]\n",
    "\n",
    "# random initialization\n",
    "    weights = []\n",
    "    for i in range(d):\n",
    "        weights.append(torch.empty((2, 2)))\n",
    "        torch.nn.init.kaiming_uniform_(weights[-1])\n",
    "        weights[-1].requires_grad = True\n",
    "\n",
    "\n",
    "    print(\"Initial weights: {}\".format(weights))\n",
    "\n",
    "    zero = torch.tensor([0.])\n",
    "\n",
    "    def model(input):\n",
    "        all_layers = {0: input}\n",
    "        for i in range(d):\n",
    "            all_layers[i+1] = linear_compare(all_layers[i], weights[i], zero)\n",
    "        return all_layers[d]\n",
    "\n",
    "    for i in range(epochs):\n",
    "    #print(\"Epoch: {}\".format(i))\n",
    "    #print(\"Start weights: {}\".format(weights))\n",
    "        done = learn(inputs, desired_outputs, model, weights, [], learning_rate=0.1,\n",
    "                        binarizatio_fn=binarization_fn, loss_gradient=loss_gradient)\n",
    "        if done == \"Done\":\n",
    "            print(\"Finished in epoch {}\".format(i))\n",
    "            break\n",
    "\n",
    "    print(\"New weights: {}\".format(weights))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in inputs:\n",
    "            print(\"{} -> {}\".format(input.numpy(), binarization_fn(model(input)).numpy()))\n",
    "\n",
    "#getting_through_layers_relu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c44354ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with model class\n",
    "import collections\n",
    "\n",
    "def getting_through_layer_model_class(d=5, epochs=10000):\n",
    "    inputs = [torch.tensor([[1., 0.], [0., 1.]], requires_grad=True)]\n",
    "    desired_outputs = [torch.tensor([[1., 0.], [0., 1.]])]\n",
    "\n",
    "    layers = [('layer_{}'.format(i+1), LinearBN(2,2, heb_lr=0.01)) for i in range(d)]\n",
    "\n",
    "    model = torch.nn.Sequential(collections.OrderedDict(layers))\n",
    "\n",
    "    print(\"Layers : {}\".format(list(model.named_children())))\n",
    "\n",
    "    print(\"Initial weights: {}\".format([l.weights for _, l in list(model.named_children())]))\n",
    "\n",
    "    for i in range(epochs):\n",
    "        done = learn_model(inputs, desired_outputs, model, learning_rate=0.1)\n",
    "        if done==\"Done\":\n",
    "            print(i)\n",
    "            break\n",
    "\n",
    "    print(\"New weights: {}\".format([l.weights for _, l in list(model.named_children())]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in inputs:\n",
    "            print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))\n",
    "\n",
    "#getting_through_layer_model_class()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c7025",
   "metadata": {},
   "source": [
    "#### Desingularization via unsupervised learning\n",
    "\n",
    "Architecture: `input_0, input_1  -> output_0, output_1` \n",
    "\n",
    "Desired behavior: no two inputs map to the same outputs\n",
    "\n",
    "Initial weights: random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0277b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.tensor([[1., 0.], [0., 1.], [1., 1.]], requires_grad=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921ae3a",
   "metadata": {},
   "source": [
    "##### Oja's rule\n",
    "\n",
    "Oja's rule fails here frequently (this is expected). \n",
    "\n",
    "For example, it produces singular outputs of the form `[[0.5005, 1.0010], [0.5005, 1.0010]]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d5ce639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desing_oja():\n",
    "    layers = [(\"layer_1\", LinearBN(2,2, oja_lr=0.1))]\n",
    "    model = torch.nn.Sequential(collections.OrderedDict(layers))\n",
    "\n",
    "    print(\"Initial weights: {}\".format([l.weights for _, l in list(model.named_children())]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in inputs:\n",
    "            print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))\n",
    "\n",
    "    limit = 2000\n",
    "\n",
    "    for i in range(limit):\n",
    "        done = learn_unsupervised(inputs, model, learning_rate=0.1)\n",
    "\n",
    "    print(\"New weights: {}\".format([l.weights for _, l in list(model.named_children())]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in inputs:\n",
    "            print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))\n",
    "\n",
    "#desing_oja()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921ae3a",
   "metadata": {},
   "source": [
    "##### Generalized Hebbian rule\n",
    "\n",
    "It does a better job than Oja's rule. However, it cannot handle a row of weights that are close to zero, because the corresponding neuron will never activate. This must be handled during initialization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d5ce639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [Parameter containing:\n",
      "tensor([[1.6816, 2.2430],\n",
      "        [1.4475, 1.3858]], requires_grad=True)]\n",
      "[[1. 0.]\n",
      " [0. 1.]] -> [[1. 1.]\n",
      " [1. 1.]]\n",
      "New weights: [Parameter containing:\n",
      "tensor([[0.5029, 1.4145],\n",
      "        [0.9963, 0.5966]], requires_grad=True)]\n",
      "[[1. 0.]\n",
      " [0. 1.]] -> [[0. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.tensor([[1., 0.], [0., 1.]], requires_grad=True)]\n",
    "\n",
    "layers = [(\"layer_1\", LinearBN(2,2, heb_lr=0.1))]\n",
    "model = torch.nn.Sequential(collections.OrderedDict(layers))\n",
    "\n",
    "print(\"Initial weights: {}\".format([l.weights for _, l in list(model.named_children())]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))\n",
    "\n",
    "limit = 100\n",
    "\n",
    "for i in range(limit):\n",
    "    done = learn_unsupervised(inputs, model, learning_rate=0.1)\n",
    "\n",
    "print(\"New weights: {}\".format([l.weights for _, l in list(model.named_children())]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c44354ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try unsupervised learning first, then supervised\n",
    "\n",
    "def getting_through_layers_unsupervised(d=6, epochs=1000):\n",
    "    inputs = [torch.tensor([[1., 0.], [0., 1.]], requires_grad=True)]\n",
    "    desired_outputs = [torch.tensor([[1., 0.], [0., 1.]])]\n",
    "\n",
    "    layers = [('layer_{}'.format(i+1), LinearBN(2,2, heb_lr=0.01)) for i in range(d)]\n",
    "\n",
    "    model = torch.nn.Sequential(collections.OrderedDict(layers))\n",
    "\n",
    "    print(\"Layers : {}\".format(list(model.named_children())))\n",
    "\n",
    "    print(\"Initial weights: {}\".format([l.weights for _, l in list(model.named_children())]))\n",
    "\n",
    "# first, train unsupervised\n",
    "    for i in range(300):\n",
    "        done = learn_unsupervised(inputs, model, learning_rate=1.)\n",
    "\n",
    "    print(\"After unsupervised training: {}\".format([l.weights for _, l in list(model.named_children())]))\n",
    "\n",
    "#set heb_lr to zero\n",
    "    for _, l in model.named_children():\n",
    "        l.heb_lr = torch.tensor([0.02])\n",
    "\n",
    "# then train supervised\n",
    "\n",
    "    for i in range(epochs):\n",
    "        done = learn_model(inputs, desired_outputs, model, learning_rate=0.1)\n",
    "        if done==\"Done\":\n",
    "            print(\"Done after {} episodes\".format(i))\n",
    "            break\n",
    "\n",
    "    print(\"New weights: {}\".format([l.weights for _, l in list(model.named_children())]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input in inputs:\n",
    "            print(\"{} -> {}\".format(input.numpy(), model(input).numpy()))\n",
    "\n",
    "#getting_through_layers_unsupervised()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c312a",
   "metadata": {},
   "source": [
    "#### MNIST with binary models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb15458",
   "metadata": {},
   "source": [
    "##### Classical approach according to pytorch tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5699db",
   "metadata": {},
   "source": [
    "Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4b23a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# binarization\n",
    "binarize_images = transforms.Compose([transforms.ToTensor(), lambda x: x>0.5, lambda x: x.float()])\n",
    "#binarize_images = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='/home/andre/data', train=True, \n",
    "                                                download=True, transform=binarize_images)\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9bffaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec8db32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img):\n",
    "    with torch.no_grad():\n",
    "        img = torchvision.utils.make_grid(img)     \n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray', vmin=0, vmax=1)\n",
    "        plt.show()\n",
    "\n",
    "imshow(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d5f8c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        #self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.pool1 = nn.Conv2d(6,6,2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
    "        self.pool2 = nn.Conv2d(12,12,2, stride=2)\n",
    "        self.fc1 = nn.Linear(192, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = MNISTNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a1564",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "*Note*: Run training again if loss is not decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c843ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a364f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cbook import report_memory\n",
    "\n",
    "\n",
    "def mnist_classical_training():\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "        running_loss = 0.\n",
    "        running_grad_conv1 = 0.\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inspect_grad = torch.abs(net.conv1.weight.grad).sum().numpy()\n",
    "\n",
    "        # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_grad_conv1 += inspect_grad\n",
    "            report_period = 200\n",
    "            if i % report_period == report_period-1:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / report_period :.3f}')\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] grad_conv1_changes: {running_grad_conv1 / report_period:.3f}')\n",
    "                running_loss = 0.0\n",
    "                running_grad_conv1 = 0.\n",
    "            \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "#mnist_classical_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "63c355c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(images):\n",
    "    with torch.no_grad():\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        print('Predicted: ', ' '.join(f'{trainset.classes[predicted[j]]:5s}||'\n",
    "                                        for j in range(4)))\n",
    "        imshow(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "25aaae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  6 - six|| 6 - six|| 6 - six|| 6 - six||\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4fdd23",
   "metadata": {},
   "source": [
    "Accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d32ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root='/home/andre/data', train=False,\n",
    "                                       download=True, transform=binarize_images)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef538518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_classical_accuracy():\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct // total} %')\n",
    "\n",
    "# mnist_classical_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb15458",
   "metadata": {},
   "source": [
    "##### Binary approach to MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58f566",
   "metadata": {},
   "source": [
    "Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d5f8c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_MNIST = 0.25\n",
    "\n",
    "class MNISTNetBN(nn.Module):\n",
    "    def __init__(self, Q=torch.tensor([0.0125]), B=B_MNIST):\n",
    "        super().__init__()\n",
    "        # wrap classes to binary\n",
    "        #Conv2dBN = bn(nn.Conv2d, Q)\n",
    "        #LinearBN = bn(nn.Linear, Q)\n",
    "        # Max pooling?\n",
    "        #MaxPool2dBN = bn(nn.MaxPool2d, Q)\n",
    "        \n",
    "        qb = {\"Q\": Q, \"B\": B}\n",
    "        self.conv1 = Conv2dBN(1, 24, 5, stride=2, **qb)\n",
    "        #self.pool1 = Conv2dBN(6,6,2, stride=2)\n",
    "        self.conv2 = Conv2dBN(24, 24, 5, stride=2, **qb)\n",
    "        #self.pool2 = Conv2dBN(12,12,2, stride=2)\n",
    "        #self.fc1 = LinearB(192, 40, **qb)\n",
    "        #self.fc2 = LinearB(40, 20, **qb)\n",
    "        self.fc3 = LinearB(384, 10, **qb)\n",
    "        #self.fc3 = LinearB(96, 10, **qb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool1(self.conv1(x))\n",
    "        # x = self.pool2(self.conv2(x))\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        #x = self.fc1(x)\n",
    "        #x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = MNISTNetBN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "07ca33db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1009, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(net.conv1.core.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b9ae5f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0204, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(net.conv2.core.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "729a96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.mean(torch.abs(net.fc1.core.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "35288c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0253, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(net.fc3.core.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "19fcea42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 24, 12, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1.], grad_fn=<Unique2Backward0>), tensor([12053,  1771]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = net.conv1(images)\n",
    "print(t.shape)\n",
    "torch.unique(t, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8d4df3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     net.conv1.core.weight = nn.Parameter(net.conv1.core.weight - 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "603c872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 24, 12, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1.], grad_fn=<Unique2Backward0>), tensor([12053,  1771]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t = net.pool1(net.conv1(images))\n",
    "t = net.conv1(images)\n",
    "print(t.shape)\n",
    "torch.unique(t, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "71b281cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1.], grad_fn=<Unique2Backward0>), tensor([1401,  135]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t = net.pool2(net.conv2(t))\n",
    "t = net.conv2(t)\n",
    "torch.unique(t, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "833c771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 384])\n"
     ]
    }
   ],
   "source": [
    "t = torch.flatten(t, 1)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "24255d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = net.fc1(t)\n",
    "# print(t.shape)\n",
    "# torch.unique(t, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cfc3d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = net.fc2(t)\n",
    "# print(t.shape)\n",
    "# torch.unique(t, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "580dcc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1.], grad_fn=<Unique2Backward0>), tensor([39,  1]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = net.fc3(t)\n",
    "print(t.shape)\n",
    "torch.unique(t, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee4ef6",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f7bf0b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 8, 2])\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "from torch import classes\n",
    "\n",
    "def labels_to_desired(labels):\n",
    "    return torch.nn.functional.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "print(labels)\n",
    "print(labels_to_desired(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "49d8c067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neq(labels_to_desired(labels), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fd9fcff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bn(grad):\n",
    "    with torch.no_grad():\n",
    "        d = float(grad.shape[0]) * float(grad.shape[1])\n",
    "        grad_flat = torch.flatten(grad)\n",
    "        s = grad_flat.sum()\n",
    "        return (s/d).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7c961124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_grad(grad, outputs, penalty = 1.):\n",
    "    filter_not_one = (torch.sum(outputs, -1) != 1.).float()\n",
    "    penalty_tensor = (1. + penalty * filter_not_one)\n",
    "    return grad * torch.reshape(penalty_tensor, penalty_tensor.shape + (1,))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a364f78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.204\n",
      "[1,  2000] grad_conv1_changes: -0.0124, 0.0842\n",
      "[1,  2000] grad_conv2_changes: -0.0005, 0.1381\n",
      "[1,  2000] grad_fc3_changes: 0.0006, 0.2793\n",
      "[1,  4000] loss: 0.178\n",
      "[1,  4000] grad_conv1_changes: -0.0001, 0.0770\n",
      "[1,  4000] grad_conv2_changes: 0.0003, 0.1217\n",
      "[1,  4000] grad_fc3_changes: 0.0002, 0.2337\n",
      "[1,  6000] loss: 0.173\n",
      "[1,  6000] grad_conv1_changes: -0.0001, 0.0799\n",
      "[1,  6000] grad_conv2_changes: 0.0001, 0.1174\n",
      "[1,  6000] grad_fc3_changes: -0.0002, 0.2289\n",
      "[1,  8000] loss: 0.177\n",
      "[1,  8000] grad_conv1_changes: -0.0000, 0.0757\n",
      "[1,  8000] grad_conv2_changes: -0.0000, 0.1178\n",
      "[1,  8000] grad_fc3_changes: 0.0000, 0.2308\n",
      "Lowered learning rate\n",
      "[1, 10000] loss: 0.158\n",
      "[1, 10000] grad_conv1_changes: -0.0005, 0.0565\n",
      "[1, 10000] grad_conv2_changes: -0.0000, 0.0998\n",
      "[1, 10000] grad_fc3_changes: -0.0007, 0.2172\n",
      "[1, 12000] loss: 0.152\n",
      "[1, 12000] grad_conv1_changes: 0.0002, 0.0439\n",
      "[1, 12000] grad_conv2_changes: -0.0000, 0.0882\n",
      "[1, 12000] grad_fc3_changes: -0.0002, 0.2103\n",
      "[1, 14000] loss: 0.147\n",
      "[1, 14000] grad_conv1_changes: 0.0001, 0.0402\n",
      "[1, 14000] grad_conv2_changes: -0.0000, 0.0807\n",
      "[1, 14000] grad_fc3_changes: -0.0003, 0.2016\n",
      "[2,  2000] loss: 0.149\n",
      "[2,  2000] grad_conv1_changes: 0.0005, 0.0370\n",
      "[2,  2000] grad_conv2_changes: -0.0001, 0.0784\n",
      "[2,  2000] grad_fc3_changes: 0.0001, 0.2027\n",
      "[2,  4000] loss: 0.154\n",
      "[2,  4000] grad_conv1_changes: 0.0002, 0.0371\n",
      "[2,  4000] grad_conv2_changes: -0.0000, 0.0776\n",
      "[2,  4000] grad_fc3_changes: -0.0002, 0.2077\n",
      "Lowered learning rate\n",
      "[2,  6000] loss: 0.138\n",
      "[2,  6000] grad_conv1_changes: -0.0006, 0.0311\n",
      "[2,  6000] grad_conv2_changes: 0.0001, 0.0670\n",
      "[2,  6000] grad_fc3_changes: -0.0005, 0.1937\n",
      "[2,  8000] loss: 0.138\n",
      "[2,  8000] grad_conv1_changes: -0.0001, 0.0293\n",
      "[2,  8000] grad_conv2_changes: -0.0001, 0.0641\n",
      "[2,  8000] grad_fc3_changes: -0.0004, 0.1943\n",
      "Lowered learning rate\n",
      "[2, 10000] loss: 0.131\n",
      "[2, 10000] grad_conv1_changes: -0.0001, 0.0272\n",
      "[2, 10000] grad_conv2_changes: 0.0000, 0.0600\n",
      "[2, 10000] grad_fc3_changes: -0.0007, 0.1878\n",
      "[2, 12000] loss: 0.129\n",
      "[2, 12000] grad_conv1_changes: -0.0000, 0.0244\n",
      "[2, 12000] grad_conv2_changes: -0.0001, 0.0562\n",
      "[2, 12000] grad_fc3_changes: -0.0003, 0.1867\n",
      "[2, 14000] loss: 0.129\n",
      "[2, 14000] grad_conv1_changes: -0.0001, 0.0238\n",
      "[2, 14000] grad_conv2_changes: 0.0000, 0.0545\n",
      "[2, 14000] grad_fc3_changes: -0.0000, 0.1854\n",
      "[3,  2000] loss: 0.128\n",
      "[3,  2000] grad_conv1_changes: -0.0000, 0.0235\n",
      "[3,  2000] grad_conv2_changes: 0.0001, 0.0530\n",
      "[3,  2000] grad_fc3_changes: -0.0001, 0.1846\n",
      "[3,  4000] loss: 0.129\n",
      "[3,  4000] grad_conv1_changes: 0.0000, 0.0231\n",
      "[3,  4000] grad_conv2_changes: -0.0000, 0.0526\n",
      "[3,  4000] grad_fc3_changes: -0.0001, 0.1850\n",
      "Lowered learning rate\n",
      "[3,  6000] loss: 0.122\n",
      "[3,  6000] grad_conv1_changes: -0.0002, 0.0220\n",
      "[3,  6000] grad_conv2_changes: -0.0001, 0.0493\n",
      "[3,  6000] grad_fc3_changes: -0.0005, 0.1745\n",
      "[3,  8000] loss: 0.129\n",
      "[3,  8000] grad_conv1_changes: -0.0000, 0.0226\n",
      "[3,  8000] grad_conv2_changes: 0.0001, 0.0504\n",
      "[3,  8000] grad_fc3_changes: 0.0002, 0.1862\n",
      "Lowered learning rate\n",
      "[3, 10000] loss: 0.119\n",
      "[3, 10000] grad_conv1_changes: -0.0000, 0.0207\n",
      "[3, 10000] grad_conv2_changes: 0.0000, 0.0473\n",
      "[3, 10000] grad_fc3_changes: -0.0005, 0.1753\n",
      "[3, 12000] loss: 0.121\n",
      "[3, 12000] grad_conv1_changes: 0.0001, 0.0210\n",
      "[3, 12000] grad_conv2_changes: -0.0001, 0.0475\n",
      "[3, 12000] grad_fc3_changes: -0.0002, 0.1755\n",
      "Lowered learning rate\n",
      "[3, 14000] loss: 0.121\n",
      "[3, 14000] grad_conv1_changes: -0.0000, 0.0211\n",
      "[3, 14000] grad_conv2_changes: 0.0002, 0.0476\n",
      "[3, 14000] grad_fc3_changes: -0.0001, 0.1779\n",
      "Lowered learning rate\n",
      "[4,  2000] loss: 0.117\n",
      "[4,  2000] grad_conv1_changes: -0.0003, 0.0203\n",
      "[4,  2000] grad_conv2_changes: 0.0001, 0.0453\n",
      "[4,  2000] grad_fc3_changes: 0.0001, 0.1691\n",
      "[4,  4000] loss: 0.115\n",
      "[4,  4000] grad_conv1_changes: 0.0002, 0.0207\n",
      "[4,  4000] grad_conv2_changes: 0.0000, 0.0460\n",
      "[4,  4000] grad_fc3_changes: 0.0001, 0.1689\n",
      "[4,  6000] loss: 0.116\n",
      "[4,  6000] grad_conv1_changes: -0.0001, 0.0205\n",
      "[4,  6000] grad_conv2_changes: -0.0003, 0.0453\n",
      "[4,  6000] grad_fc3_changes: -0.0001, 0.1669\n",
      "Lowered learning rate\n",
      "[4,  8000] loss: 0.118\n",
      "[4,  8000] grad_conv1_changes: 0.0000, 0.0203\n",
      "[4,  8000] grad_conv2_changes: 0.0000, 0.0460\n",
      "[4,  8000] grad_fc3_changes: 0.0019, 0.1734\n",
      "Lowered learning rate\n",
      "[4, 10000] loss: 0.116\n",
      "[4, 10000] grad_conv1_changes: 0.0000, 0.0200\n",
      "[4, 10000] grad_conv2_changes: 0.0000, 0.0452\n",
      "[4, 10000] grad_fc3_changes: -0.0071, 0.1694\n",
      "[4, 12000] loss: 0.118\n",
      "[4, 12000] grad_conv1_changes: -0.0004, 0.0204\n",
      "[4, 12000] grad_conv2_changes: 0.0000, 0.0451\n",
      "[4, 12000] grad_fc3_changes: -0.0005, 0.1700\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def mnist_BN_training(lr_MNIST = 0.001, penalty=2.):\n",
    "    \n",
    "    lr_running = lr_MNIST \n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr_MNIST)\n",
    "    num_lower_lr = 10\n",
    "\n",
    "    for epoch in range(100):  # loop over the dataset multiple times\n",
    "        previous_running_loss = 0.\n",
    "        running_loss = 0.\n",
    "        running_grad = {\"conv1\": [0., 0.], \"conv2\": [0.,0.],\n",
    "                        #\"pool1\": [0., 0.], \"pool2\": [0.,0.],     \n",
    "                        #\"fc1\": [0., 0.], \n",
    "                        \"fc3\": [0., 0.]}\n",
    "\n",
    "        if num_lower_lr==0:\n",
    "            break\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            one_hot_labels = labels_to_desired(labels)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                grad = neq(outputs, one_hot_labels)\n",
    "                grad_loss = improve_grad(grad, outputs, penalty=penalty)\n",
    "            \n",
    "            outputs.backward(grad_loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            #if net.fc3.core.weight.grad is not None:\n",
    "            #    print(torch.abs(net.fc3.core.weight.grad))\n",
    "\n",
    "            # Debug\n",
    "            with torch.no_grad():\n",
    "                for layer in running_grad:\n",
    "                    layer_grad = getattr(net, layer).core.weight.grad\n",
    "                    num_layer_parameters = 1.\n",
    "                    for d in layer_grad.shape:\n",
    "                        num_layer_parameters *= d\n",
    "                    inspect_grad_abs = torch.abs(layer_grad).sum().numpy()\n",
    "                    inspect_grad = layer_grad.sum().numpy()\n",
    "                    #(lr_MNIST/B_MNIST) *\n",
    "                    running_grad[layer][0] += (1/B_MNIST) *(inspect_grad/num_layer_parameters)/batch_size\n",
    "                    running_grad[layer][1] += (1/B_MNIST) *(inspect_grad_abs/num_layer_parameters)/batch_size\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "            running_loss += loss_bn(grad_loss)     \n",
    "            report_period = 2000\n",
    "            if i % report_period == report_period-1:    \n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / report_period :.3f}')\n",
    "                for layer, l in running_grad.items():\n",
    "                    l = [x * 1/float(report_period) for x in l]\n",
    "                    print(f'[{epoch + 1}, {i + 1:5d}] grad_{layer}_changes: {l[0] :.4f}, {l[1] :.4f}')\n",
    "                \n",
    "                #reset\n",
    "                if running_loss>previous_running_loss + 0.001 and previous_running_loss!=0.:\n",
    "                    lr_running = 0.5 * lr_running\n",
    "                    optimizer = torch.optim.SGD(net.parameters(), lr=lr_running)\n",
    "                    num_lower_lr -= 1\n",
    "                    if num_lower_lr == 0:\n",
    "                        break\n",
    "                    print(\"Lowered learning rate\")\n",
    "                    \n",
    "                previous_running_loss = running_loss\n",
    "                running_loss = 0.\n",
    "                for layer in running_grad:\n",
    "                    running_grad[layer] = [0.,0.]\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "mnist_BN_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005be3d",
   "metadata": {},
   "source": [
    "Accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0910ab17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_correct(t, labels):\n",
    "    filter_only_one = torch.sum(t, -1)==1\n",
    "    filter_one_and_correct = torch.argmax(t[filter_only_one], -1) == torch.argmax(labels_to_desired(labels)[filter_only_one], -1)\n",
    "    correct = filter_one_and_correct.sum().item()\n",
    "    false = filter_only_one.sum().item() - correct\n",
    "    return correct, false\n",
    "\n",
    "num_correct(t, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ef538518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts for the labels 0,1,..,9: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}\n",
      "\n",
      "Number of predictions returned per input (1 is best, 0 means no prediction): {0: 1922, 1: 6736, 2: 1253, 3: 87, 4: 2, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0}\n",
      "\n",
      "Analysis when only one prediction is returned: {'correct': 6202, 'false': 534}\n",
      "\n",
      "Accuracy on only one prediction: 0.9207244655581948\n",
      "Analysis when only one prediction is returned per label: {0: {'correct': 0, 'false': 0}, 1: {'correct': 0, 'false': 0}, 2: {'correct': 0, 'false': 0}, 3: {'correct': 0, 'false': 0}, 4: {'correct': 0, 'false': 0}, 5: {'correct': 0, 'false': 0}, 6: {'correct': 0, 'false': 0}, 7: {'correct': 0, 'false': 0}, 8: {'correct': 0, 'false': 0}, 9: {'correct': 6202, 'false': 534}}\n",
      "\n",
      "For which labels 0,1,..,9 did the model not predict: {0: 108, 1: 139, 2: 164, 3: 220, 4: 192, 5: 247, 6: 178, 7: 186, 8: 213, 9: 275}\n",
      "\n",
      "Confusion matrix\n",
      "[[747   0  11   5   1  10   6   3   7   3]\n",
      " [  0 930   4   4   5   3   2  11  10   4]\n",
      " [  6   1 638  14   2   3   5  16   5   1]\n",
      " [  2   0  12 575   0  17   0   1  15   7]\n",
      " [  1   2  10   2 598   8   4   3  10  29]\n",
      " [  8   0   1   6   4 408   8   0  16   4]\n",
      " [  2   0   9   3  11   9 639   1   8   4]\n",
      " [  1   1  11  11   5   5   1 652   5  17]\n",
      " [  4   1  15  12   4  19   2   4 521   7]\n",
      " [  1   0   3   2  22   5   2   6   9 494]]\n"
     ]
    }
   ],
   "source": [
    "def mnist_bn_statistics():\n",
    "    total_stats = {k: 0 for k in range(10)}\n",
    "    stat = {k: 0 for k in range(11)}\n",
    "    one_and_correct = {\"correct\": 0, \"false\": 0}\n",
    "    missed_by_no_prediction  = {k: 0 for k in range(10)}\n",
    "    one_detailed = {k: {\"correct\": 0, \"false\": 0} for k in range(10)}\n",
    "    confusion_matrix = np.zeros((10, 10), dtype=np.int16)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            for k in stat:\n",
    "                filter_k = torch.sum(outputs, -1)==k\n",
    "                stat[k] += (filter_k).sum().item()\n",
    "                \n",
    "                if k==0:\n",
    "                    for label in missed_by_no_prediction:\n",
    "                        missed_by_no_prediction[label] += (labels[filter_k]==label).sum().item()\n",
    "\n",
    "                if k==1:\n",
    "                    for prediction in one_detailed:\n",
    "                        outputs_one = outputs[filter_k]\n",
    "                        labels_one = labels[filter_k]\n",
    "                        filter_prediction = torch.argmax(outputs_one, -1) == prediction\n",
    "                        labels_prediction = labels_one[filter_prediction]\n",
    "\n",
    "                        num_results_label = num_correct(outputs_one[filter_prediction], labels_prediction)\n",
    "                        one_detailed[label][\"correct\"] += num_results_label[0]\n",
    "                        one_detailed[label][\"false\"] += num_results_label[1]\n",
    "                        \n",
    "                        for label in range(10):\n",
    "                            confusion_matrix[prediction, label] += (labels_prediction==label).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "            for k in total_stats:     \n",
    "                total_stats[k] += (labels == k).sum().item()\n",
    "\n",
    "            num_results = num_correct(outputs, labels)\n",
    "            one_and_correct[\"correct\"] += num_results[0]\n",
    "            one_and_correct[\"false\"] += num_results[1]\n",
    "\n",
    "    print(\"Total counts for the labels 0,1,..,9: {}\\n\".format(total_stats))\n",
    "    print(\"Number of predictions returned per input (1 is best, 0 means no prediction): {}\\n\".format(stat)) \n",
    "    print(\"Analysis when only one prediction is returned: {}\\n\".format(one_and_correct))\n",
    "    print(f'Accuracy on only one prediction: {one_and_correct[\"correct\"]/(stat[1]):.1f}')\n",
    "    print(\"Analysis when only one prediction is returned per label: {}\\n\".format(one_detailed))\n",
    "    print(\"For which labels 0,1,..,9 did the model not predict: {}\\n\".format(missed_by_no_prediction))\n",
    "    print(\"Confusion matrix\")\n",
    "    print(confusion_matrix)\n",
    "\n",
    "mnist_bn_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a919a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 1.12 Kernel",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "96bacbb292411d9e8d48d6a003550589e7a6b28e03eb0e879664ddbf5f968d66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
